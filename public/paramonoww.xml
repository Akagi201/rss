<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <atom:link href="https://nitter.freedit.eu/paramonoww/rss" rel="self" type="application/rss+xml" />
    <title>Pavel Paramonov / @paramonoww</title>
    <link>https://nitter.freedit.eu/paramonoww</link>
    <description>Twitter feed for: @paramonoww. Generated by nitter.freedit.eu
</description>
    <language>en-us</language>
    <ttl>40</ttl>
    <image>
      <title>Pavel Paramonov / @paramonoww</title>
      <link>https://nitter.freedit.eu/paramonoww</link>
      <url>https://nitter.freedit.eu/pic/pbs.twimg.com%2Fprofile_images%2F1873333991572688896%2F7HtZMgae_400x400.jpg</url>
      <width>128</width>
      <height>128</height>
    </image>
      <item>
        <title>RT by @paramonoww: How can we make the use of web2 data in web3 actually private and verifiable?

Many people who claim that web3 is the new internet define it with the phrase &quot;read, write, own.&quot; The &quot;read&quot; and &quot;write&quot; parts are clear, but when it comes to &quot;own&quot; in terms of data, we hardly own anything today.

User data is often stolen by corporations and used in ways that benefit them; we don’t truly own anything on the internet. 

However, we can&apos;t just shift to a world where only web3 exists without sharing anything. No, we still need to share, but only what&apos;s necessary.

As someone with a weaker passport, I&apos;m stuck applying for e-visas and submitting endless details about myself to prove I&apos;m eligible for specific visas. And I always end up asking myself:

• Why should I share my entire bank statement when they only need to confirm a specific income level?

• Why should I provide the exact hotel reservation instead of just proving I&apos;ve booked a hotel in this country?

• Why do I have to submit my full passport details when all they need is to verify my permanent residence in my current country?

There are two main concerns here: services know far more than they need to, and the data you&apos;re providing isn&apos;t private. But how does this relate to security and privacy in crypto?

1. Web3 is not gonna make it without web2 data.

As most of you know, smart contracts essentially have no idea how much BTC, ETH, SOL, or any other asset costs. This task is delegated to oracles, which constantly post public data from the outside world to the smart contract.

In the Ethereum world, this role is almost monopolized by @chainlink with their oracle networks to ensure we don&apos;t rely on a single node. So, we really do need web2 data for more use cases beyond just knowing the price of certain assets.

However, this only applies to public data. What if I want to securely connect my bank account or Telegram account and share sensitive information that isn&apos;t publicly available but is private to me?

The first thought is: how can we bring this data onto a blockchain with proof that the private data is secure?

Unfortunately, it doesn’t work that way because servers don’t sign the responses they send, so you can’t reliably verify something like that in smart contracts.

The protocol that secures communication over a computer network is called TLS: Transport Layer Security. Even if you haven&apos;t heard of it, you use it daily. For example, while reading this article, you&apos;ll see the &quot;https://&quot; in your browser&apos;s address bar.

If you tried accessing the website with an &quot;http://&quot; connection (without the &quot;s&quot;), your browser would warn you that the connection isn&apos;t secure. The &quot;s&quot; in the link stands for TLS, which secures your connection, ensuring privacy and preventing anyone from stealing the data you&apos;re transmitting.

2. The connection is already secure, can&apos;t we just transport and use it in the web3?

As I mentioned before, we face a verifiability problem: servers don’t sign the responses they send, so we can&apos;t really verify the data.

Even if a data source agrees to share its data, the standard TLS protocol can&apos;t prove its authenticity to others. Simply passing a response isn’t enough: clients can easily alter the data locally, sharing those responses fully exposes them, risking user privacy.

One approach to the verifiability problem is an enhanced version of TLS called zkTLS.

The working mechanism of zkTLS is similar to TLS but slightly different, here&apos;s how it works:

• You visit a website through a secure TLS connection and send the required request.

• zkTLS generates a zk proof that verifies the data while revealing only the specific details the user wants to prove, keeping everything else private.

• The generated zk proof is then used by other apps to confirm and verify that the provided information is correct.

When I say zkTLS, I&apos;m referring to projects utilizing zkTLS, but there are different approaches to data verifiability using various solutions: 

1. TEE (Trusted Execution Environment)
2. MPC (Multi-Party Computation)
3. Proxy

Interestingly, each approach introduces its own set of unique use cases. So, how do they differ?

3. Why isn&apos;t there a single standard for zkTLS? How are they different?

zkTLS isn&apos;t a single technology because verifying private web data without exposing it can be approached from multiple angles, each with its own trade-offs. The core idea is to extend TLS with proofs, but how you generate and validate those proofs depends on the underlying mechanism.

As I mentioned before, three main approaches are using TEE-TLS, MPC-TLS, or Proxy-TLS.

TEE relies on specialized hardware, like Intel SGX or AWS Nitro Enclaves, to create a secure &quot;black box&quot; where data can be processed and proofs generated. The hardware ensures the data stays private and computations are tamper-proof.

In a TEE-based zkTLS setup, the TEE runs the protocol, proving the TLS session&apos;s execution and content. The verifier is the TEE itself, so trust depends on the TEE&apos;s manufacturer and its resistance to attacks. This approach is efficient with low computational and network overhead. 

However, it has a major flaw: you have to trust the hardware manufacturer, and vulnerabilities in TEEs (like side-channel attacks) can break the whole system.

Proxy-TLS and MPC-TLS are the most widely adopted approaches due to their broader range of use cases. Projects like @OpacityNetwork and @reclaimprotocol, that are built on @eigenlayer, leverage these models to ensure data security and privacy along with an additional layer of economic security.

Let&apos;s see how secure these solutions are, which use cases zkTLS protocols enable, and what&apos;s already live today.

4. What&apos;s so special about MPC-TLS and Opacity Network?

During the TLS Handshake (where a client and server agree on how to securely communicate by sharing encryption keys), the website&apos;s role remains unchanged, but the browser&apos;s process does something different.

Instead of generating its own secret key, it uses a network of nodes to create a multiparty secret key via MPC. This key performs the handshake for the browser, ensuring that no single entity knows the shared key.

Encryption and decryption require cooperation among all nodes and the browser, with each adding or removing their part of the encryption sequentially before data reaches or leaves the website. MPC-TLS provides strong security and can be distributed so no one group has all the power.

Opacity Network enhances the classic @tlsnotary framework by adding safeguards to minimize trust issues. It employs multiple security measures like:

1. On-chain verification of web2 account IDs
2. Commit scheme
3. Reveal scheme
4. Random MPC-network sampling
5. Verifiable log of attempts

Account IDs, being static in web2 systems, allow for proof by committee where ten different nodes must confirm ownership. This links the account to a unique wallet, preventing repeated tries with different wallets to find a colluding node. 

You can see how Opacity works in detail down below:

Opacity nodes operate within a TEE, making collusion almost impossible if the TEE is secure. Beyond TEEs, Opacity also uses Eigenlayer to leverage an AVS, requiring nodes to restake 32 stETH, with immediate slashing for misconduct, avoiding delays associated with cooldowns.

You can see that Opacity uses both MPC and TEE, but because MPC is used for zkTLS while TEE is used basically for node security, it&apos;s still called MPC-TLS.

However, if the TEEs were to fail, it could enable a node to engage in collusion within the MPC. That&apos;s one of the reasons why an additional economic security layer is needed to prevent this behavior.

That&apos;s also why Opacity is developing a whistleblower mechanism where users who can prove that a notary has acted improperly will be rewarded with a share of the penalty imposed on the notary&apos;s stake.

Due to its simplicity of integration, security, and the privacy it offers, Opacity has attracted various protocols to integrate it into their products across consumer, DeFi, and AI agent sectors.

The team from @earnos_io is developing a platform where brands can reward users for engagement or task completion. EarnOS uses Opacity’s tech to prove traits via popular apps without revealing personal info, letting brands target audiences while users keep privacy and earn rewards.

Opacity is also integrated into the @daylightenergy_ protocol, developing a decentralized electric utility network where users can earn rewards for contributing to clean energy solutions. Thanks to Opacity, users can prove energy device ownership on-chain without specialized hardware.

Opacity can even be integrated with AI agents, bringing more verifiability and transparency to a field that currently faces significant challenges. zkTLS was recently integrated into @elizaOS, allowing for verifiable AI interactions without privacy loss. 

However, TEE-TLS and MPC-TLS are only two variations of zkTLS, there&apos;s also a third one called Proxy-TLS, with the Reclaim Network being the most famous representation of this model. So, how is it different in terms of tech from the other two variations, and which use cases can be enabled by Proxy-TLS?

5. What’s so special about Proxy-TLS and Reclaim Protocol?

HTTPS proxies, common on the internet, forward encrypted traffic without accessing its content. In the zkTLS proxy model, it works almost the same with slight additions:

• The browser sends requests to the website through a proxy, which also handles the website&apos;s responses.

• The proxy sees all encrypted exchanges and attests to their authenticity, noting whether each is a request or response.

• The browser then generates a zk proof which states that it can encrypt this data with a shared key without revealing the key and shows the result.

• This works because it&apos;s nearly impossible to create a fake key that turns the data into anything sensible, so just showing you can decrypt it is enough.

Revealing the key would compromise all prior messages, including sensitive data like usernames and passwords. Proxy-TLS is pretty fast, affordable, and handles large data volumes well, making it ideal for high-throughput settings. 

The majority of servers don&apos;t restrict access based on varying IP addresses, making this method pretty widely applicable.

Reclaim Protocol uses Proxy-TLS for efficient data verification and employs proxies to bypass Web2 firewalls preventing large-scale proxy blocking. 

Here&apos;s how it works:

The main problem here is collusion: if the user and attestor collude, they can sign basically anything and act maliciously. To mitigate this, Reclaim incorporates a subset of validators chosen to introduce randomness and block such exploits.

Reclaim uses Eigen&apos;s AVS to decentralize the validation of the data. EigenLayer operators can act as attestors, but they will need to deploy their own AVS to specify the attestation logic for their service.

Reclaim is a platform enabling unique use cases like importing ride-sharing data for transportation apps, bridging off-chain data for blockchain economics, verifying identities with national ID data, creating custom data solutions via developer tools, and more.

The Reclaim ecosystem is home to 20+ projects, but I&apos;d like to highlight 4 of them in the money markets, digital identity, consumer, and hiring sectors.

@3janexyz is the first credit-based money market on Base, offering secured credit lines to crypto users by assessing their creditworthiness and future cash flows, using both on-chain and off-chain financial data.

3Jane uses Reclaim&apos;s proxy model to verify credit data from VantageScore, Cred, Coinbase, and Plaid, ensuring privacy of this data.

Another use for credit scores with zkTLS is through @zkme_&apos;s feature, zkCreditScore. It uses Reclaim Protocol to get your US credit score securely with zkTLS. This lets zkMe check a user&apos;s credit score and make unique soulbound tokens (SBTs) to store this data.

Can there be any other use cases besides credit scores? Of course, there are. 

We can take @zkp2p as an example, which is a consumer goods marketplace that leverages Reclaim for verifying users&apos; Ticketmaster data as well as verifying user payments. 

At the same time, @bondexapp, which is one of the most popular job boards in crypto, uses Reclaim for getting proof of work of profiles, verifying that the data is real, private, and verifiable.

Looking at the use cases possible via zkTLS, the ability to verify TLS transcripts on-chain is already unlocking numerous new functionalities, allowing users to control their own data without needing permission from large corporations. 

More importantly, zkTLS is made to ensure that your personal data is not used against you. So, where is this headed?

6. Is zkTLS here to stay?

There is still work to be done, but different zkTLS protocols are already introducing new use cases that redistribute power back to the users. 

@Tim_Roughgarden on the a16z crypto podcast highlighted that zk proofs, proposed in 1985, only gained popularity with blockchain applications, thanks to hundreds of developers working to reduce proof size and costs.

And now, contributions from the blockchain industry are finding uses in other areas beyond just crypto itself.

I expect a similar story to play out with zkTLS, starting with implementation in Web3 and then extending beyond that, because, as I said before, currently, we &quot;read&quot; and &quot;write,&quot; but we are hardly protected and hardly &quot;own&quot; even our own data.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>How can we make the use of web2 data in web3 actually private and verifiable?<br>
<br>
Many people who claim that web3 is the new internet define it with the phrase "read, write, own." The "read" and "write" parts are clear, but when it comes to "own" in terms of data, we hardly own anything today.<br>
<br>
User data is often stolen by corporations and used in ways that benefit them; we don’t truly own anything on the internet. <br>
<br>
However, we can't just shift to a world where only web3 exists without sharing anything. No, we still need to share, but only what's necessary.<br>
<br>
As someone with a weaker passport, I'm stuck applying for e-visas and submitting endless details about myself to prove I'm eligible for specific visas. And I always end up asking myself:<br>
<br>
• Why should I share my entire bank statement when they only need to confirm a specific income level?<br>
<br>
• Why should I provide the exact hotel reservation instead of just proving I've booked a hotel in this country?<br>
<br>
• Why do I have to submit my full passport details when all they need is to verify my permanent residence in my current country?<br>
<br>
There are two main concerns here: services know far more than they need to, and the data you're providing isn't private. But how does this relate to security and privacy in crypto?<br>
<br>
1. Web3 is not gonna make it without web2 data.<br>
<br>
As most of you know, smart contracts essentially have no idea how much BTC, ETH, SOL, or any other asset costs. This task is delegated to oracles, which constantly post public data from the outside world to the smart contract.<br>
<br>
In the Ethereum world, this role is almost monopolized by <a href="https://nitter.freedit.eu/chainlink" title="Chainlink">@chainlink</a> with their oracle networks to ensure we don't rely on a single node. So, we really do need web2 data for more use cases beyond just knowing the price of certain assets.<br>
<br>
However, this only applies to public data. What if I want to securely connect my bank account or Telegram account and share sensitive information that isn't publicly available but is private to me?<br>
<br>
The first thought is: how can we bring this data onto a blockchain with proof that the private data is secure?<br>
<br>
Unfortunately, it doesn’t work that way because servers don’t sign the responses they send, so you can’t reliably verify something like that in smart contracts.<br>
<br>
The protocol that secures communication over a computer network is called TLS: Transport Layer Security. Even if you haven't heard of it, you use it daily. For example, while reading this article, you'll see the "https://" in your browser's address bar.<br>
<br>
If you tried accessing the website with an "http://" connection (without the "s"), your browser would warn you that the connection isn't secure. The "s" in the link stands for TLS, which secures your connection, ensuring privacy and preventing anyone from stealing the data you're transmitting.<br>
<br>
2. The connection is already secure, can't we just transport and use it in the web3?<br>
<br>
As I mentioned before, we face a verifiability problem: servers don’t sign the responses they send, so we can't really verify the data.<br>
<br>
Even if a data source agrees to share its data, the standard TLS protocol can't prove its authenticity to others. Simply passing a response isn’t enough: clients can easily alter the data locally, sharing those responses fully exposes them, risking user privacy.<br>
<br>
One approach to the verifiability problem is an enhanced version of TLS called zkTLS.<br>
<br>
The working mechanism of zkTLS is similar to TLS but slightly different, here's how it works:<br>
<br>
• You visit a website through a secure TLS connection and send the required request.<br>
<br>
• zkTLS generates a zk proof that verifies the data while revealing only the specific details the user wants to prove, keeping everything else private.<br>
<br>
• The generated zk proof is then used by other apps to confirm and verify that the provided information is correct.<br>
<br>
When I say zkTLS, I'm referring to projects utilizing zkTLS, but there are different approaches to data verifiability using various solutions: <br>
<br>
1. TEE (Trusted Execution Environment)<br>
2. MPC (Multi-Party Computation)<br>
3. Proxy<br>
<br>
Interestingly, each approach introduces its own set of unique use cases. So, how do they differ?<br>
<br>
3. Why isn't there a single standard for zkTLS? How are they different?<br>
<br>
zkTLS isn't a single technology because verifying private web data without exposing it can be approached from multiple angles, each with its own trade-offs. The core idea is to extend TLS with proofs, but how you generate and validate those proofs depends on the underlying mechanism.<br>
<br>
As I mentioned before, three main approaches are using TEE-TLS, MPC-TLS, or Proxy-TLS.<br>
<br>
TEE relies on specialized hardware, like Intel SGX or AWS Nitro Enclaves, to create a secure "black box" where data can be processed and proofs generated. The hardware ensures the data stays private and computations are tamper-proof.<br>
<br>
In a TEE-based zkTLS setup, the TEE runs the protocol, proving the TLS session's execution and content. The verifier is the TEE itself, so trust depends on the TEE's manufacturer and its resistance to attacks. This approach is efficient with low computational and network overhead. <br>
<br>
However, it has a major flaw: you have to trust the hardware manufacturer, and vulnerabilities in TEEs (like side-channel attacks) can break the whole system.<br>
<br>
Proxy-TLS and MPC-TLS are the most widely adopted approaches due to their broader range of use cases. Projects like <a href="https://nitter.freedit.eu/OpacityNetwork" title="Opacity Network">@OpacityNetwork</a> and <a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a>, that are built on <a href="https://nitter.freedit.eu/eigenlayer" title="EigenLayer">@eigenlayer</a>, leverage these models to ensure data security and privacy along with an additional layer of economic security.<br>
<br>
Let's see how secure these solutions are, which use cases zkTLS protocols enable, and what's already live today.<br>
<br>
4. What's so special about MPC-TLS and Opacity Network?<br>
<br>
During the TLS Handshake (where a client and server agree on how to securely communicate by sharing encryption keys), the website's role remains unchanged, but the browser's process does something different.<br>
<br>
Instead of generating its own secret key, it uses a network of nodes to create a multiparty secret key via MPC. This key performs the handshake for the browser, ensuring that no single entity knows the shared key.<br>
<br>
Encryption and decryption require cooperation among all nodes and the browser, with each adding or removing their part of the encryption sequentially before data reaches or leaves the website. MPC-TLS provides strong security and can be distributed so no one group has all the power.<br>
<br>
Opacity Network enhances the classic <a href="https://nitter.freedit.eu/tlsnotary" title="TLSNotary">@tlsnotary</a> framework by adding safeguards to minimize trust issues. It employs multiple security measures like:<br>
<br>
1. On-chain verification of web2 account IDs<br>
2. Commit scheme<br>
3. Reveal scheme<br>
4. Random MPC-network sampling<br>
5. Verifiable log of attempts<br>
<br>
Account IDs, being static in web2 systems, allow for proof by committee where ten different nodes must confirm ownership. This links the account to a unique wallet, preventing repeated tries with different wallets to find a colluding node. <br>
<br>
You can see how Opacity works in detail down below:<br>
<br>
Opacity nodes operate within a TEE, making collusion almost impossible if the TEE is secure. Beyond TEEs, Opacity also uses Eigenlayer to leverage an AVS, requiring nodes to restake 32 stETH, with immediate slashing for misconduct, avoiding delays associated with cooldowns.<br>
<br>
You can see that Opacity uses both MPC and TEE, but because MPC is used for zkTLS while TEE is used basically for node security, it's still called MPC-TLS.<br>
<br>
However, if the TEEs were to fail, it could enable a node to engage in collusion within the MPC. That's one of the reasons why an additional economic security layer is needed to prevent this behavior.<br>
<br>
That's also why Opacity is developing a whistleblower mechanism where users who can prove that a notary has acted improperly will be rewarded with a share of the penalty imposed on the notary's stake.<br>
<br>
Due to its simplicity of integration, security, and the privacy it offers, Opacity has attracted various protocols to integrate it into their products across consumer, DeFi, and AI agent sectors.<br>
<br>
The team from <a href="https://nitter.freedit.eu/earnos_io" title="EarnOS">@earnos_io</a> is developing a platform where brands can reward users for engagement or task completion. EarnOS uses Opacity’s tech to prove traits via popular apps without revealing personal info, letting brands target audiences while users keep privacy and earn rewards.<br>
<br>
Opacity is also integrated into the <a href="https://nitter.freedit.eu/daylightenergy_" title="Daylight Energy 🔆">@daylightenergy_</a> protocol, developing a decentralized electric utility network where users can earn rewards for contributing to clean energy solutions. Thanks to Opacity, users can prove energy device ownership on-chain without specialized hardware.<br>
<br>
Opacity can even be integrated with AI agents, bringing more verifiability and transparency to a field that currently faces significant challenges. zkTLS was recently integrated into <a href="https://nitter.freedit.eu/elizaOS" title="elizaOS">@elizaOS</a>, allowing for verifiable AI interactions without privacy loss. <br>
<br>
However, TEE-TLS and MPC-TLS are only two variations of zkTLS, there's also a third one called Proxy-TLS, with the Reclaim Network being the most famous representation of this model. So, how is it different in terms of tech from the other two variations, and which use cases can be enabled by Proxy-TLS?<br>
<br>
5. What’s so special about Proxy-TLS and Reclaim Protocol?<br>
<br>
HTTPS proxies, common on the internet, forward encrypted traffic without accessing its content. In the zkTLS proxy model, it works almost the same with slight additions:<br>
<br>
• The browser sends requests to the website through a proxy, which also handles the website's responses.<br>
<br>
• The proxy sees all encrypted exchanges and attests to their authenticity, noting whether each is a request or response.<br>
<br>
• The browser then generates a zk proof which states that it can encrypt this data with a shared key without revealing the key and shows the result.<br>
<br>
• This works because it's nearly impossible to create a fake key that turns the data into anything sensible, so just showing you can decrypt it is enough.<br>
<br>
Revealing the key would compromise all prior messages, including sensitive data like usernames and passwords. Proxy-TLS is pretty fast, affordable, and handles large data volumes well, making it ideal for high-throughput settings. <br>
<br>
The majority of servers don't restrict access based on varying IP addresses, making this method pretty widely applicable.<br>
<br>
Reclaim Protocol uses Proxy-TLS for efficient data verification and employs proxies to bypass Web2 firewalls preventing large-scale proxy blocking. <br>
<br>
Here's how it works:<br>
<br>
The main problem here is collusion: if the user and attestor collude, they can sign basically anything and act maliciously. To mitigate this, Reclaim incorporates a subset of validators chosen to introduce randomness and block such exploits.<br>
<br>
Reclaim uses Eigen's AVS to decentralize the validation of the data. EigenLayer operators can act as attestors, but they will need to deploy their own AVS to specify the attestation logic for their service.<br>
<br>
Reclaim is a platform enabling unique use cases like importing ride-sharing data for transportation apps, bridging off-chain data for blockchain economics, verifying identities with national ID data, creating custom data solutions via developer tools, and more.<br>
<br>
The Reclaim ecosystem is home to 20+ projects, but I'd like to highlight 4 of them in the money markets, digital identity, consumer, and hiring sectors.<br>
<br>
<a href="https://nitter.freedit.eu/3janexyz" title="3Jane">@3janexyz</a> is the first credit-based money market on Base, offering secured credit lines to crypto users by assessing their creditworthiness and future cash flows, using both on-chain and off-chain financial data.<br>
<br>
3Jane uses Reclaim's proxy model to verify credit data from VantageScore, Cred, Coinbase, and Plaid, ensuring privacy of this data.<br>
<br>
Another use for credit scores with zkTLS is through <a href="https://nitter.freedit.eu/zkme_" title="zkMe">@zkme_</a>'s feature, zkCreditScore. It uses Reclaim Protocol to get your US credit score securely with zkTLS. This lets zkMe check a user's credit score and make unique soulbound tokens (SBTs) to store this data.<br>
<br>
Can there be any other use cases besides credit scores? Of course, there are. <br>
<br>
We can take <a href="https://nitter.freedit.eu/zkp2p" title="ZKP2P">@zkp2p</a> as an example, which is a consumer goods marketplace that leverages Reclaim for verifying users' Ticketmaster data as well as verifying user payments. <br>
<br>
At the same time, <a href="https://nitter.freedit.eu/bondexapp" title="Bondex">@bondexapp</a>, which is one of the most popular job boards in crypto, uses Reclaim for getting proof of work of profiles, verifying that the data is real, private, and verifiable.<br>
<br>
Looking at the use cases possible via zkTLS, the ability to verify TLS transcripts on-chain is already unlocking numerous new functionalities, allowing users to control their own data without needing permission from large corporations. <br>
<br>
More importantly, zkTLS is made to ensure that your personal data is not used against you. So, where is this headed?<br>
<br>
6. Is zkTLS here to stay?<br>
<br>
There is still work to be done, but different zkTLS protocols are already introducing new use cases that redistribute power back to the users. <br>
<br>
<a href="https://nitter.freedit.eu/Tim_Roughgarden" title="Tim Roughgarden">@Tim_Roughgarden</a> on the a16z crypto podcast highlighted that zk proofs, proposed in 1985, only gained popularity with blockchain applications, thanks to hundreds of developers working to reduce proof size and costs.<br>
<br>
And now, contributions from the blockchain industry are finding uses in other areas beyond just crypto itself.<br>
<br>
I expect a similar story to play out with zkTLS, starting with implementation in Web3 and then extending beyond that, because, as I said before, currently, we "read" and "write," but we are hardly protected and hardly "own" even our own data.</p>
<img src="https://nitter.freedit.eu/pic/media%2FGjgs6wAagAAWAZ1.jpg" style="max-width:250px;" />
<img src="https://nitter.freedit.eu/pic/media%2FGjguSa9aYAAhYP9.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Tue, 11 Feb 2025 14:17:37 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1889317871551180955#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1889317871551180955#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: Great thread, owning the data is the key element now</title>
        <dc:creator>@Srijith_Padmesh</dc:creator>
        <description><![CDATA[<p>Great thread, owning the data is the key element now</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1889317871551180955#m">nitter.freedit.eu/paramonoww/status/1889317871551180955#m</a></p>]]></description>
        <pubDate>Tue, 11 Feb 2025 15:43:06 GMT</pubDate>
        <guid>https://nitter.freedit.eu/Srijith_Padmesh/status/1889339386111693019#m</guid>
        <link>https://nitter.freedit.eu/Srijith_Padmesh/status/1889339386111693019#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: Building the foundation for private, verifiable web2 data in crypto. @OpacityNetwork and @reclaimprotocol are showing what&apos;s possible with EigenLayer.</title>
        <dc:creator>@eigenlayer</dc:creator>
        <description><![CDATA[<p>Building the foundation for private, verifiable web2 data in crypto. <a href="https://nitter.freedit.eu/OpacityNetwork" title="Opacity Network">@OpacityNetwork</a> and <a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a> are showing what's possible with EigenLayer.</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1889317871551180955#m">nitter.freedit.eu/paramonoww/status/1889317871551180955#m</a></p>]]></description>
        <pubDate>Tue, 11 Feb 2025 15:15:59 GMT</pubDate>
        <guid>https://nitter.freedit.eu/eigenlayer/status/1889332560804839555#m</guid>
        <link>https://nitter.freedit.eu/eigenlayer/status/1889332560804839555#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: zkTLS is one of the most powerful and useful primitives we&apos;ve seen come out of crypto.

and proves crypto primitives are not only for onchain finance.

@earnos_io - proving app engagement for brand rewards while preserving privacy (@OpacityNetwork)
@daylightenergy_ - verifying energy device ownership without exposing personal data (@OpacityNetwork)
@OpenAI / other LLMs + @elizaOS - making AI interactions verifiable while maintaining privacy (@OpacityNetwork)
Earnifi - enabling arbitrary verification of bank account details with privacy (@reclaimprotocol)
@3janexyz - verifying credit data for DeFi lending (@reclaimprotocol)
@zkme_ - converting credit scores into private soulbound tokens (@reclaimprotocol)
@zkp2p - verifying @Ticketmaster data and payments for marketplace (@reclaimprotocol)
@bondexapp - verifying work history proofs for job applications (@reclaimprotocol)</title>
        <dc:creator>@dabit3</dc:creator>
        <description><![CDATA[<p>zkTLS is one of the most powerful and useful primitives we've seen come out of crypto.<br>
<br>
and proves crypto primitives are not only for onchain finance.<br>
<br>
<a href="https://nitter.freedit.eu/earnos_io" title="EarnOS">@earnos_io</a> - proving app engagement for brand rewards while preserving privacy (<a href="https://nitter.freedit.eu/OpacityNetwork" title="Opacity Network">@OpacityNetwork</a>)<br>
<a href="https://nitter.freedit.eu/daylightenergy_" title="Daylight Energy 🔆">@daylightenergy_</a> - verifying energy device ownership without exposing personal data (<a href="https://nitter.freedit.eu/OpacityNetwork" title="Opacity Network">@OpacityNetwork</a>)<br>
<a href="https://nitter.freedit.eu/OpenAI" title="OpenAI">@OpenAI</a> / other LLMs + <a href="https://nitter.freedit.eu/elizaOS" title="elizaOS">@elizaOS</a> - making AI interactions verifiable while maintaining privacy (<a href="https://nitter.freedit.eu/OpacityNetwork" title="Opacity Network">@OpacityNetwork</a>)<br>
Earnifi - enabling arbitrary verification of bank account details with privacy (<a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a>)<br>
<a href="https://nitter.freedit.eu/3janexyz" title="3Jane">@3janexyz</a> - verifying credit data for DeFi lending (<a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a>)<br>
<a href="https://nitter.freedit.eu/zkme_" title="zkMe">@zkme_</a> - converting credit scores into private soulbound tokens (<a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a>)<br>
<a href="https://nitter.freedit.eu/zkp2p" title="ZKP2P">@zkp2p</a> - verifying <a href="https://nitter.freedit.eu/Ticketmaster" title="Ticketmaster">@Ticketmaster</a> data and payments for marketplace (<a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a>)<br>
<a href="https://nitter.freedit.eu/bondexapp" title="Bondex">@bondexapp</a> - verifying work history proofs for job applications (<a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a>)</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1889317871551180955#m">nitter.freedit.eu/paramonoww/status/1889317871551180955#m</a></p>]]></description>
        <pubDate>Tue, 11 Feb 2025 15:18:44 GMT</pubDate>
        <guid>https://nitter.freedit.eu/dabit3/status/1889333254261559694#m</guid>
        <link>https://nitter.freedit.eu/dabit3/status/1889333254261559694#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: Own your web2 data: financial info, credit score, driver / house host ratings, identity, Amazon transaction history etc etc 
 
How @reclaimprotocol and @OpacityNetwork help you reclaim your data opaquely ( privately). 

Built on @eigenlayer!</title>
        <dc:creator>@sreeramkannan</dc:creator>
        <description><![CDATA[<p>Own your web2 data: financial info, credit score, driver / house host ratings, identity, Amazon transaction history etc etc <br>
 <br>
How <a href="https://nitter.freedit.eu/reclaimprotocol" title="Reclaim Protocol">@reclaimprotocol</a> and <a href="https://nitter.freedit.eu/OpacityNetwork" title="Opacity Network">@OpacityNetwork</a> help you reclaim your data opaquely ( privately). <br>
<br>
Built on <a href="https://nitter.freedit.eu/eigenlayer" title="EigenLayer">@eigenlayer</a>!</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1889317871551180955#m">nitter.freedit.eu/paramonoww/status/1889317871551180955#m</a></p>]]></description>
        <pubDate>Tue, 11 Feb 2025 16:35:13 GMT</pubDate>
        <guid>https://nitter.freedit.eu/sreeramkannan/status/1889352502312927516#m</guid>
        <link>https://nitter.freedit.eu/sreeramkannan/status/1889352502312927516#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: How Gigagas Rollups Work with @rise_chain as the main study. Feel free to read my guest article below.

That&apos;s an interesting approach to scaling, but, obviously, not every app will require such speed.

This is the final series on exploring different types of rollups.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>How Gigagas Rollups Work with <a href="https://nitter.freedit.eu/rise_chain" title="RISE Chain">@rise_chain</a> as the main study. Feel free to read my guest article below.<br>
<br>
That's an interesting approach to scaling, but, obviously, not every app will require such speed.<br>
<br>
This is the final series on exploring different types of rollups.</p>
<p><a href="https://nitter.freedit.eu/2077Research/status/1886892073615339927#m">nitter.freedit.eu/2077Research/status/1886892073615339927#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGjALCyXa4AAjB1D.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Wed, 05 Feb 2025 06:08:18 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1887020404642078745#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1887020404642078745#m</link>
      </item>
      <item>
        <title>How Native Rollups work, feel free to read my guest article down below.

Yes, this is one of the approaches to interop that can allow composability at the validator level.

However, we still need to solve interoperability for other rollups, which will 100% not be native.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>How Native Rollups work, feel free to read my guest article down below.<br>
<br>
Yes, this is one of the approaches to interop that can allow composability at the validator level.<br>
<br>
However, we still need to solve interoperability for other rollups, which will 100% not be native.</p>
<p><a href="https://nitter.freedit.eu/2077Research/status/1883980391301755100#m">nitter.freedit.eu/2077Research/status/1883980391301755100#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGiWk3kHaYAQ25Qc.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Tue, 28 Jan 2025 04:41:07 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1884099362370076831#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1884099362370076831#m</link>
      </item>
      <item>
        <title>I&apos;m hiring a researcher to work closely with me on both public and private research projects. 

This role is ideal for someone who is highly self-organized, possesses excellent communication skills, and has the ability to write interesting and insightful content.  

Hard Skills: 
• Researcher first, writer second.

• 3+ published tech and product articles that are insightful and engaging, not just restructured tech docs.

• Focus on infrastructure primarily for EVM and Solana.

• Brief knowledge of the top 50 protocols in the crypto space. 

• Experience with Data Analytics is a bonus.  

Soft Skills: 
• Self-organized with a keen attention to detail, understanding that details separate good research from great research. 

• Ability to manage multiple projects in a fast-paced environment. 

• Clear and transparent communicator, which is possibly one of the most critical aspects. 

• Capable of suggesting own ideas, especially when you feel like something is not right.

Opportunities: 
• Work with me and tier-1 protocols in the blockchain space.

• Learn about and implement a research and writing pipeline.

• Remote work, if you&apos;re based in Asian time zone, it&apos;s even better, but not critical.

• Contract or full-time position; I&apos;m open to suggestions on this, but note that work never ends: manage your time wisely.  

If you&apos;re interested, please send me a DM with a brief introduction, your writings, and your resume if you have one.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>I'm hiring a researcher to work closely with me on both public and private research projects. <br>
<br>
This role is ideal for someone who is highly self-organized, possesses excellent communication skills, and has the ability to write interesting and insightful content.  <br>
<br>
Hard Skills: <br>
• Researcher first, writer second.<br>
<br>
• 3+ published tech and product articles that are insightful and engaging, not just restructured tech docs.<br>
<br>
• Focus on infrastructure primarily for EVM and Solana.<br>
<br>
• Brief knowledge of the top 50 protocols in the crypto space. <br>
<br>
• Experience with Data Analytics is a bonus.  <br>
<br>
Soft Skills: <br>
• Self-organized with a keen attention to detail, understanding that details separate good research from great research. <br>
<br>
• Ability to manage multiple projects in a fast-paced environment. <br>
<br>
• Clear and transparent communicator, which is possibly one of the most critical aspects. <br>
<br>
• Capable of suggesting own ideas, especially when you feel like something is not right.<br>
<br>
Opportunities: <br>
• Work with me and tier-1 protocols in the blockchain space.<br>
<br>
• Learn about and implement a research and writing pipeline.<br>
<br>
• Remote work, if you're based in Asian time zone, it's even better, but not critical.<br>
<br>
• Contract or full-time position; I'm open to suggestions on this, but note that work never ends: manage your time wisely.  <br>
<br>
If you're interested, please send me a DM with a brief introduction, your writings, and your resume if you have one.</p>]]></description>
        <pubDate>Fri, 24 Jan 2025 07:59:14 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1882699667269669212#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1882699667269669212#m</link>
      </item>
      <item>
        <title>How Booster Rollups work primarily enables Ethereum&apos;s horizontal scaling</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>How Booster Rollups work primarily enables Ethereum's horizontal scaling</p>
<p><a href="https://nitter.freedit.eu/2077Research/status/1881786222961430840#m">nitter.freedit.eu/2077Research/status/1881786222961430840#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGh34ZS0bsAAlMbu.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Wed, 22 Jan 2025 05:16:18 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1881933887254962343#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1881933887254962343#m</link>
      </item>
      <item>
        <title>Hyperliquid is a pretty weird platform. I connected my wallet, bridged USDC from Arbitrum, bought HYPE on spot, and it was okay.

After two days, I went to the website, and it said my account was flagged as high risk, while on all other dapps, it was never recognized as such. 

I messaged them through email, but they didn&apos;t respond. I opened a ticket on Discord, and the guy sent me a template response. After my question, I got muted for 7 days?

I can&apos;t swap, deposit, transfer, or withdraw, so I can&apos;t do anything with the money I deposited.

The fix was using @Hypurrfun to swap HYPE for USDC and transfer from spot to perps, and then using @deBridgeFinance to bridge from Hyperliquid to L2.

Thanks to @KamBenbrik and @cp0xdotcom for helping with the issue.

I understand that even if my address was marked as high risk, why would you let me deposit on the platform but not allow me to withdraw? 

Okay, even if this happened, should I be muted for 7 days after simply asking how I can get my money back if I can&apos;t do it via your UI?</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>Hyperliquid is a pretty weird platform. I connected my wallet, bridged USDC from Arbitrum, bought HYPE on spot, and it was okay.<br>
<br>
After two days, I went to the website, and it said my account was flagged as high risk, while on all other dapps, it was never recognized as such. <br>
<br>
I messaged them through email, but they didn't respond. I opened a ticket on Discord, and the guy sent me a template response. After my question, I got muted for 7 days?<br>
<br>
I can't swap, deposit, transfer, or withdraw, so I can't do anything with the money I deposited.<br>
<br>
The fix was using <a href="https://nitter.freedit.eu/Hypurrfun" title="Hfun">@Hypurrfun</a> to swap HYPE for USDC and transfer from spot to perps, and then using <a href="https://nitter.freedit.eu/deBridgeFinance" title="deBridge">@deBridgeFinance</a> to bridge from Hyperliquid to L2.<br>
<br>
Thanks to <a href="https://nitter.freedit.eu/KamBenbrik" title="Kam 🌑">@KamBenbrik</a> and <a href="https://nitter.freedit.eu/cp0xdotcom" title="cp0x.com">@cp0xdotcom</a> for helping with the issue.<br>
<br>
I understand that even if my address was marked as high risk, why would you let me deposit on the platform but not allow me to withdraw? <br>
<br>
Okay, even if this happened, should I be muted for 7 days after simply asking how I can get my money back if I can't do it via your UI?</p>
<img src="https://nitter.freedit.eu/pic/media%2FGhz_BsOaoAAIx9C.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Tue, 21 Jan 2025 11:20:58 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1881663274837803418#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1881663274837803418#m</link>
      </item>
      <item>
        <title>I was glad to research the based rollups functionality of @Spire_Labs and @taikoxyz and write a small article about it for the 2077 team.

If you&apos;ve always wondered about based rollups, that piece would be helpful. The next parts will include booster rollups and native rollups.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>I was glad to research the based rollups functionality of <a href="https://nitter.freedit.eu/Spire_Labs" title="Spire🗼">@Spire_Labs</a> and <a href="https://nitter.freedit.eu/taikoxyz" title="Taiko.eth 🥁">@taikoxyz</a> and write a small article about it for the 2077 team.<br>
<br>
If you've always wondered about based rollups, that piece would be helpful. The next parts will include booster rollups and native rollups.</p>
<p><a href="https://nitter.freedit.eu/2077Research/status/1879976056750502327#m">nitter.freedit.eu/2077Research/status/1879976056750502327#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGheWke5acAAYCaa.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Fri, 17 Jan 2025 06:23:09 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1880138774413471916#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1880138774413471916#m</link>
      </item>
      <item>
        <title>Just went through the latest 100 blocks of @EclipseFND and found out that over 90% of each block consists of &quot;SetComputeUnitLimit&quot; transactions, which simply set 60,000 compute units as the max amount to consume.

Swaps, transfers, and most user activity represent less than 1%.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>Just went through the latest 100 blocks of <a href="https://nitter.freedit.eu/EclipseFND" title="Eclipse (🐮,🌑)">@EclipseFND</a> and found out that over 90% of each block consists of "SetComputeUnitLimit" transactions, which simply set 60,000 compute units as the max amount to consume.<br>
<br>
Swaps, transfers, and most user activity represent less than 1%.</p>
<img src="https://nitter.freedit.eu/pic/media%2FGhayQ-rboAAzaCi.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Thu, 16 Jan 2025 13:46:31 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1879887963968209220#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1879887963968209220#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: A well written tweet on @Eigen_da, a must read.</title>
        <dc:creator>@Satyams246</dc:creator>
        <description><![CDATA[<p>A well written tweet on <a href="https://nitter.freedit.eu/eigen_da" title="EigenDA">@Eigen_da</a>, a must read.</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m">nitter.freedit.eu/paramonoww/status/1879500495641006082#m</a></p>]]></description>
        <pubDate>Thu, 16 Jan 2025 04:10:16 GMT</pubDate>
        <guid>https://nitter.freedit.eu/Satyams246/status/1879742943923458269#m</guid>
        <link>https://nitter.freedit.eu/Satyams246/status/1879742943923458269#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: Reminder that EigenDA single handedly has processed:

Bitcoin
Ethereum
Solana
Celo
Celestia
Avalanche
BNB chain
Fantom
Polygon
Mantle
Base
Arbitrum
Optimism
XAI games
Blast
Scroll
ZKSync
Mode
Taiko

blocks all at once and still operated at &lt;54% capacity.

EigenDA is Hyperscale.</title>
        <dc:creator>@dabit3</dc:creator>
        <description><![CDATA[<p>Reminder that EigenDA single handedly has processed:<br>
<br>
Bitcoin<br>
Ethereum<br>
Solana<br>
Celo<br>
Celestia<br>
Avalanche<br>
BNB chain<br>
Fantom<br>
Polygon<br>
Mantle<br>
Base<br>
Arbitrum<br>
Optimism<br>
XAI games<br>
Blast<br>
Scroll<br>
ZKSync<br>
Mode<br>
Taiko<br>
<br>
blocks all at once and still operated at &lt;54% capacity.<br>
<br>
EigenDA is Hyperscale.</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m">nitter.freedit.eu/paramonoww/status/1879500495641006082#m</a></p>]]></description>
        <pubDate>Thu, 16 Jan 2025 00:10:55 GMT</pubDate>
        <guid>https://nitter.freedit.eu/dabit3/status/1879682707883745763#m</guid>
        <link>https://nitter.freedit.eu/dabit3/status/1879682707883745763#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: #EigenDA the #rollup lever.

- The #EigenLayer AVS for Data Availability.
- Explained clearly and understandably.
- Perfectly summarized in the diagram.
- Well done. 👍</title>
        <dc:creator>@criptosimpatic</dc:creator>
        <description><![CDATA[<p><a href="https://nitter.freedit.eu/search?q=%23EigenDA">#EigenDA</a> the <a href="https://nitter.freedit.eu/search?q=%23rollup">#rollup</a> lever.<br>
<br>
- The <a href="https://nitter.freedit.eu/search?q=%23EigenLayer">#EigenLayer</a> AVS for Data Availability.<br>
- Explained clearly and understandably.<br>
- Perfectly summarized in the diagram.<br>
- Well done. 👍</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m">nitter.freedit.eu/paramonoww/status/1879500495641006082#m</a></p>]]></description>
        <pubDate>Thu, 16 Jan 2025 00:12:06 GMT</pubDate>
        <guid>https://nitter.freedit.eu/criptosimpatic/status/1879683007633842502#m</guid>
        <link>https://nitter.freedit.eu/criptosimpatic/status/1879683007633842502#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: @eigen_da is the natural place for the L2s and L3s priced out of Ethereum&apos;s blob market to direct their data availability demand.</title>
        <dc:creator>@punk5736</dc:creator>
        <description><![CDATA[<p><a href="https://nitter.freedit.eu/eigen_da" title="EigenDA">@eigen_da</a> is the natural place for the L2s and L3s priced out of Ethereum's blob market to direct their data availability demand.</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m">nitter.freedit.eu/paramonoww/status/1879500495641006082#m</a></p>]]></description>
        <pubDate>Wed, 15 Jan 2025 18:26:28 GMT</pubDate>
        <guid>https://nitter.freedit.eu/punk5736/status/1879596025637466519#m</guid>
        <link>https://nitter.freedit.eu/punk5736/status/1879596025637466519#m</link>
      </item>
      <item>
        <title>RT by @paramonoww: EigenDA visualization and breakdown 👇

Great build up of introducing DA and EigenDA&apos;s solution, worth the read</title>
        <dc:creator>@mattmurrs</dc:creator>
        <description><![CDATA[<p>EigenDA visualization and breakdown 👇<br>
<br>
Great build up of introducing DA and EigenDA's solution, worth the read</p>
<p><a href="https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m">nitter.freedit.eu/paramonoww/status/1879500495641006082#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGhV-8YNXMAAGQ5z.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Wed, 15 Jan 2025 15:16:59 GMT</pubDate>
        <guid>https://nitter.freedit.eu/mattmurrs/status/1879548339118825528#m</guid>
        <link>https://nitter.freedit.eu/mattmurrs/status/1879548339118825528#m</link>
      </item>
      <item>
        <title>Data Availability is more important than you think.

Most people view the DA Layer as temporary storage for data access by users and apps. True, but that overlooks key details: 

• why is temporary storage needed? 
• who needs this storage? 
• what kind of data needs to be stored?
• what if data is not stored correctly?
• why can&apos;t we just use Ethereum for all those purposes?

External DA solutions like @eigen_da can boost speed and throughput and lower costs while still offering strong security guarantees. Data Availability is a complex topic involving scalability, speed, economics, security, and architecture. So, why should we care?

1. Why does Data Availability matter?

The data availability problem simply asks:

&quot;How can we ensure the data needed to reconstruct the blockchain&apos;s state is accessible?&quot;

As most people already know, state is simply all the data stored on the blockchain at a given moment, which is constantly updated. That&apos;s also called liveness. Without this data, verifying transactions becomes impossible and opens the door for malicious block proposers.

Ethereum addresses this by requiring full nodes to download each block, verifying transactions by re-executing them against a local state copy. It does not work the same for rollups, obviously, because it sacrifices a lot of speed, but we still need to ensure data is available to maintain security.

But what data even is that? I referred to the state lots of times, so the data that needs to be available is &quot;state data,&quot; a.k.a. transaction history. If data is not live, you sacrifice security and risk that your rollup won&apos;t be re-org resistant.

Reorgs can occur during L1 block finalization, which means that Ethereum might split into different versions due to malicious acts.

A rollup might align with a fork instead of the canonical chain until the correct chain is settled. Moreover, users might face censorship because of the weak DA layer where their transactions won&apos;t be included.

This not only poses technical challenges like data loss but also affects the UX/UI of user-facing applications.

The Data Availability problem first refers to security rather than speed and scalability.

2. What are the benefits of using an external DA layer like EigenDA?

Many rollups use Ethereum for data availability due to its strong security. So, is there any reason to use EigenDA? Yes, there are lots of them. 

A rollup can be divided into multiple layers, including the settlement layer, DA layer, execution layer, etc. The overall idea of modular blockchain is to divide it into multiple components to maximize efficiency.

However, relying on Ethereum as a DA layer does not make much sense in cases where you&apos;re aiming not only for security but also for reducing costs. The surge of L2 solutions on Ethereum led to high competition for blockspace, making transactions costly.

By March 2024, L2s spent over $37.4 million for storing and accessing data on Ethereum. That changed after the introduction of blobs with a 99% reduction in price. Well, the price was fixed, but fees are still uncertain due to Ethereum congestion.

Rollups like @0xMantle and @megaeth_labs use EigenDA because Ethereum is not a comfortable place to achieve high throughput. That means that Ethereum rollups must compete with other L1 transactions for limited blockspace, which can increase storage costs in the process.

• Ethereum blocks can handle 768kB from blobs plus other data, reaching about 2.5 MB total. 
• With blocks every 12 seconds, this gives a bandwidth of 0.2 MB/s.
• It is pretty slow and approximately equals 9,000 TPS. 

That is the TPS not for one rollup, but for all rollups that use Ethereum as a DA Layer. And yes, that&apos;s pretty slow + there is no horizontal scaling available (compute growth is not proportional to node growth).

Moreover, rollups that use EigenDA have the autonomy to handle their own state updates, in contrast to Ethereum-based rollups, which depend on Ethereum&apos;s network for managing changes.

3. How does EigenDA fix those problems?

• As I said before, rollups like MegaETH with their 100,000+ TPS are not able to use Ethereum for DA purposes. 
• EigenDA currently has 15MB/s of write throughput. 
• That means that it can handle up to 654,000 TPS with compression.

It is pretty suitable for chains that want to onboard apps where high throughput is needed, like APIs, different backends, HFT, etc. But why is it possible?

• When a rollup&apos;s sequencer sends a block to EigenDA, the disperser (an EigenDA component) erasure codes this blob into chunks, which are basically smaller parts of the blob.

• Those chunks are sent to different storage operators, getting confirmation that they have stored the data. 

• The thing is that each unique storage operator processes their own unique set of chunks, so there are no two operators who process the same chunks. 

This approach makes it possible for horizontal scaling to exist: more operators -&gt; more chunks can be processed -&gt; higher throughput -&gt; higher TPS.

The average cost of storing 1MB of data in EigenDA is approximately $0.05 at the time of writing, which makes it pretty cheap and convenient for rollups who want to store their data and make it accessible at any given point in time.

At the beginning, I mentioned that data availability is security first. But how does EigenDA handle security in that case? 

EigenDA is an AVS on EigenLayer, which means that EigenDA has its own operator set with stake delegated to those operators to economically secure EigenDA. There are over 200 operators with more than $15 billion in stake combined. So, this system is pretty secure.

Current DA systems often merge data accessibility verification with data ordering, creating complex setups. While verifying data can be done in parallel, ordering it for consensus is slow. 

This is useful for systems managing their own data order but unnecessary for systems like EigenDA, which leverage Ethereum&apos;s existing ordering mechanisms.

EigenDA achieves greater speed and efficiency in data storage, thanks to its use of erasure coding and the KZG commitment scheme, which allows for storing only the minimal required data.

4. How does EigenDA work?

The EigenDA architecture might seem pretty complex at the beginning, but in the end, everything makes sense.

1) Rollup&apos;s sequencer sends a batch of L2 transactions in the form of a blob to the disperser.

2) Disperser divides this blob into small pieces (chunks) and creates two types of proofs: KZG commitment proof and KZG multi-reveal proofs.

3) All chunks are uniquely distributed across the EigenDA Operator Set (operators running EigenDA node software and having stake delegated to them).

4) Operators also receive the commitment proof to verify chunks against it using multi-reveal proofs.

5) Operators send signatures to the disperser that they have successfully verified the chunks.

6) Disperser aggregates received signatures, registers the blob on-chain by sending a transaction to the EigenDA Manager Contract.

7) Manager Contract checks with the Operator Registry that signatures are verified correctly.

8) Rollup sequencer sends an EigenDA blob ID to its inbox contract on Ethereum.

9) Inbox contract checks with the Manager Contract if the blob ID is indeed available.

10) If available, the blob ID is added to the inbox contract, and Ethereum’s rollup view is updated. If not, a malicious operator is slashed.

One important feature is that blobs are erasure coded at a 4.54 ratio. That means that the system provides more redundancy, which means the system can recover from more simultaneous node or storage failures without losing data.

The main reason to use erasure coding is to allow a single operator to store only one chunk, significantly reducing costs. As more operators join, individual costs drop.

5. What is the relationship between EigenDA and Ethereum consensus? Can’t rollup just use EigenDA?

Rollup can&apos;t use only EigenDA because it still needs to settle somewhere. And using EigenDA, you can only settle transactions on Ethereum. 

EigenDA does not have its own consensus mechanism; it uses Ethereum&apos;s security and validator set through restaking. This means that a rollup or an end user relies on Ethereum&apos;s consensus to ensure that data is correctly ordered and finalized.

• The rollup determines the agreed-upon sequence of transactions. 
• It then uses EigenDA for storage where transactions are posted in batches. 
• The rollup posts the state roots to Ethereum for finalization. 
• After the transactions are sequenced and recorded on the base layer, they are permanently added to the rollup chain, becoming irreversible (finalized) after a certain period.

6. If the DA layer is not secure enough, what happens?

There are two big problems a lot of teams are working on: re-org protection and censorship resistance. 

Security against censorship and reorgs requires a robust DA layer where economic security prevents majority control. The primary design concern is the cost of consensus attacks.

As the old saying goes: &quot;To minimize malicious behavior, we need to make it unprofitable.&quot; 

A blockchain is economically secure when attack costs greatly exceed potential gains. Verifying proofs on Ethereum enhances rollup security against censorship but doesn&apos;t address all issues with data availability. 

If the DA layer is compromised, the rollup remains vulnerable since Ethereum&apos;s smart contract only checks attestations. This can lead to liveness issues where new blocks can&apos;t be produced, illustrating that using one blockchain for data limits security assurances from another.

Before EigenDA, there were data availability committees which look pretty similar to dPoS. EigenDA is different because it&apos;s open for everyone who wants to run node software or delegate their assets. It also offers more security because operators can actually be slashed if acting maliciously.

7. Which apps need to access the DA network besides the rollup and Ethereum themselves?

Well, there are lots of them. DEXs, DeFi protocols, NFT marketplaces, DAOs, staking protocols, and interoperability protocols all require full transaction history for various purposes like liquidity tracking, financial calculations, authenticity verification, compliance, governance, reward calculation, and secure messaging.

@alt_layer uses EigenDA for rollups launched on their platform, @Calderaxyz lets you launch a rollup within 1 minute with EigenDA, etc. There are lots of different interesting use cases.

There has been a lot of &quot;monolithic vs. modular&quot; discussions over the past year, but in the Ethereum landscape, there has been more modular stuff winning.

Proposer-Builder Separation (PBS), DA layers, different sequencing modules, finality layers: it seems like every big part of the Ethereum ecosystem is in the process of being split into multiple ones and maximizing the efficiency of each part. 

As I&apos;ve said a lot in my previous writings, the future of millions of general-purpose rollups and app-specific rollups is inevitable, so if it&apos;s inevitable, let&apos;s make it efficient.</title>
        <dc:creator>@paramonoww</dc:creator>
        <description><![CDATA[<p>Data Availability is more important than you think.<br>
<br>
Most people view the DA Layer as temporary storage for data access by users and apps. True, but that overlooks key details: <br>
<br>
• why is temporary storage needed? <br>
• who needs this storage? <br>
• what kind of data needs to be stored?<br>
• what if data is not stored correctly?<br>
• why can't we just use Ethereum for all those purposes?<br>
<br>
External DA solutions like <a href="https://nitter.freedit.eu/eigen_da" title="EigenDA">@eigen_da</a> can boost speed and throughput and lower costs while still offering strong security guarantees. Data Availability is a complex topic involving scalability, speed, economics, security, and architecture. So, why should we care?<br>
<br>
1. Why does Data Availability matter?<br>
<br>
The data availability problem simply asks:<br>
<br>
"How can we ensure the data needed to reconstruct the blockchain's state is accessible?"<br>
<br>
As most people already know, state is simply all the data stored on the blockchain at a given moment, which is constantly updated. That's also called liveness. Without this data, verifying transactions becomes impossible and opens the door for malicious block proposers.<br>
<br>
Ethereum addresses this by requiring full nodes to download each block, verifying transactions by re-executing them against a local state copy. It does not work the same for rollups, obviously, because it sacrifices a lot of speed, but we still need to ensure data is available to maintain security.<br>
<br>
But what data even is that? I referred to the state lots of times, so the data that needs to be available is "state data," a.k.a. transaction history. If data is not live, you sacrifice security and risk that your rollup won't be re-org resistant.<br>
<br>
Reorgs can occur during L1 block finalization, which means that Ethereum might split into different versions due to malicious acts.<br>
<br>
A rollup might align with a fork instead of the canonical chain until the correct chain is settled. Moreover, users might face censorship because of the weak DA layer where their transactions won't be included.<br>
<br>
This not only poses technical challenges like data loss but also affects the UX/UI of user-facing applications.<br>
<br>
The Data Availability problem first refers to security rather than speed and scalability.<br>
<br>
2. What are the benefits of using an external DA layer like EigenDA?<br>
<br>
Many rollups use Ethereum for data availability due to its strong security. So, is there any reason to use EigenDA? Yes, there are lots of them. <br>
<br>
A rollup can be divided into multiple layers, including the settlement layer, DA layer, execution layer, etc. The overall idea of modular blockchain is to divide it into multiple components to maximize efficiency.<br>
<br>
However, relying on Ethereum as a DA layer does not make much sense in cases where you're aiming not only for security but also for reducing costs. The surge of L2 solutions on Ethereum led to high competition for blockspace, making transactions costly.<br>
<br>
By March 2024, L2s spent over $37.4 million for storing and accessing data on Ethereum. That changed after the introduction of blobs with a 99% reduction in price. Well, the price was fixed, but fees are still uncertain due to Ethereum congestion.<br>
<br>
Rollups like <a href="https://nitter.freedit.eu/0xMantle" title="Mantle Network">@0xMantle</a> and <a href="https://nitter.freedit.eu/megaeth_labs" title="MegaETH">@megaeth_labs</a> use EigenDA because Ethereum is not a comfortable place to achieve high throughput. That means that Ethereum rollups must compete with other L1 transactions for limited blockspace, which can increase storage costs in the process.<br>
<br>
• Ethereum blocks can handle 768kB from blobs plus other data, reaching about 2.5 MB total. <br>
• With blocks every 12 seconds, this gives a bandwidth of 0.2 MB/s.<br>
• It is pretty slow and approximately equals 9,000 TPS. <br>
<br>
That is the TPS not for one rollup, but for all rollups that use Ethereum as a DA Layer. And yes, that's pretty slow + there is no horizontal scaling available (compute growth is not proportional to node growth).<br>
<br>
Moreover, rollups that use EigenDA have the autonomy to handle their own state updates, in contrast to Ethereum-based rollups, which depend on Ethereum's network for managing changes.<br>
<br>
3. How does EigenDA fix those problems?<br>
<br>
• As I said before, rollups like MegaETH with their 100,000+ TPS are not able to use Ethereum for DA purposes. <br>
• EigenDA currently has 15MB/s of write throughput. <br>
• That means that it can handle up to 654,000 TPS with compression.<br>
<br>
It is pretty suitable for chains that want to onboard apps where high throughput is needed, like APIs, different backends, HFT, etc. But why is it possible?<br>
<br>
• When a rollup's sequencer sends a block to EigenDA, the disperser (an EigenDA component) erasure codes this blob into chunks, which are basically smaller parts of the blob.<br>
<br>
• Those chunks are sent to different storage operators, getting confirmation that they have stored the data. <br>
<br>
• The thing is that each unique storage operator processes their own unique set of chunks, so there are no two operators who process the same chunks. <br>
<br>
This approach makes it possible for horizontal scaling to exist: more operators -&gt; more chunks can be processed -&gt; higher throughput -&gt; higher TPS.<br>
<br>
The average cost of storing 1MB of data in EigenDA is approximately $0.05 at the time of writing, which makes it pretty cheap and convenient for rollups who want to store their data and make it accessible at any given point in time.<br>
<br>
At the beginning, I mentioned that data availability is security first. But how does EigenDA handle security in that case? <br>
<br>
EigenDA is an AVS on EigenLayer, which means that EigenDA has its own operator set with stake delegated to those operators to economically secure EigenDA. There are over 200 operators with more than $15 billion in stake combined. So, this system is pretty secure.<br>
<br>
Current DA systems often merge data accessibility verification with data ordering, creating complex setups. While verifying data can be done in parallel, ordering it for consensus is slow. <br>
<br>
This is useful for systems managing their own data order but unnecessary for systems like EigenDA, which leverage Ethereum's existing ordering mechanisms.<br>
<br>
EigenDA achieves greater speed and efficiency in data storage, thanks to its use of erasure coding and the KZG commitment scheme, which allows for storing only the minimal required data.<br>
<br>
4. How does EigenDA work?<br>
<br>
The EigenDA architecture might seem pretty complex at the beginning, but in the end, everything makes sense.<br>
<br>
1) Rollup's sequencer sends a batch of L2 transactions in the form of a blob to the disperser.<br>
<br>
2) Disperser divides this blob into small pieces (chunks) and creates two types of proofs: KZG commitment proof and KZG multi-reveal proofs.<br>
<br>
3) All chunks are uniquely distributed across the EigenDA Operator Set (operators running EigenDA node software and having stake delegated to them).<br>
<br>
4) Operators also receive the commitment proof to verify chunks against it using multi-reveal proofs.<br>
<br>
5) Operators send signatures to the disperser that they have successfully verified the chunks.<br>
<br>
6) Disperser aggregates received signatures, registers the blob on-chain by sending a transaction to the EigenDA Manager Contract.<br>
<br>
7) Manager Contract checks with the Operator Registry that signatures are verified correctly.<br>
<br>
8) Rollup sequencer sends an EigenDA blob ID to its inbox contract on Ethereum.<br>
<br>
9) Inbox contract checks with the Manager Contract if the blob ID is indeed available.<br>
<br>
10) If available, the blob ID is added to the inbox contract, and Ethereum’s rollup view is updated. If not, a malicious operator is slashed.<br>
<br>
One important feature is that blobs are erasure coded at a 4.54 ratio. That means that the system provides more redundancy, which means the system can recover from more simultaneous node or storage failures without losing data.<br>
<br>
The main reason to use erasure coding is to allow a single operator to store only one chunk, significantly reducing costs. As more operators join, individual costs drop.<br>
<br>
5. What is the relationship between EigenDA and Ethereum consensus? Can’t rollup just use EigenDA?<br>
<br>
Rollup can't use only EigenDA because it still needs to settle somewhere. And using EigenDA, you can only settle transactions on Ethereum. <br>
<br>
EigenDA does not have its own consensus mechanism; it uses Ethereum's security and validator set through restaking. This means that a rollup or an end user relies on Ethereum's consensus to ensure that data is correctly ordered and finalized.<br>
<br>
• The rollup determines the agreed-upon sequence of transactions. <br>
• It then uses EigenDA for storage where transactions are posted in batches. <br>
• The rollup posts the state roots to Ethereum for finalization. <br>
• After the transactions are sequenced and recorded on the base layer, they are permanently added to the rollup chain, becoming irreversible (finalized) after a certain period.<br>
<br>
6. If the DA layer is not secure enough, what happens?<br>
<br>
There are two big problems a lot of teams are working on: re-org protection and censorship resistance. <br>
<br>
Security against censorship and reorgs requires a robust DA layer where economic security prevents majority control. The primary design concern is the cost of consensus attacks.<br>
<br>
As the old saying goes: "To minimize malicious behavior, we need to make it unprofitable." <br>
<br>
A blockchain is economically secure when attack costs greatly exceed potential gains. Verifying proofs on Ethereum enhances rollup security against censorship but doesn't address all issues with data availability. <br>
<br>
If the DA layer is compromised, the rollup remains vulnerable since Ethereum's smart contract only checks attestations. This can lead to liveness issues where new blocks can't be produced, illustrating that using one blockchain for data limits security assurances from another.<br>
<br>
Before EigenDA, there were data availability committees which look pretty similar to dPoS. EigenDA is different because it's open for everyone who wants to run node software or delegate their assets. It also offers more security because operators can actually be slashed if acting maliciously.<br>
<br>
7. Which apps need to access the DA network besides the rollup and Ethereum themselves?<br>
<br>
Well, there are lots of them. DEXs, DeFi protocols, NFT marketplaces, DAOs, staking protocols, and interoperability protocols all require full transaction history for various purposes like liquidity tracking, financial calculations, authenticity verification, compliance, governance, reward calculation, and secure messaging.<br>
<br>
<a href="https://nitter.freedit.eu/alt_layer" title="AltLayer">@alt_layer</a> uses EigenDA for rollups launched on their platform, <a href="https://nitter.freedit.eu/Calderaxyz" title="Caldera">@Calderaxyz</a> lets you launch a rollup within 1 minute with EigenDA, etc. There are lots of different interesting use cases.<br>
<br>
There has been a lot of "monolithic vs. modular" discussions over the past year, but in the Ethereum landscape, there has been more modular stuff winning.<br>
<br>
Proposer-Builder Separation (PBS), DA layers, different sequencing modules, finality layers: it seems like every big part of the Ethereum ecosystem is in the process of being split into multiple ones and maximizing the efficiency of each part. <br>
<br>
As I've said a lot in my previous writings, the future of millions of general-purpose rollups and app-specific rollups is inevitable, so if it's inevitable, let's make it efficient.</p>
<img src="https://nitter.freedit.eu/pic/media%2FGhVMSH1bcAAQfSY.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Wed, 15 Jan 2025 12:06:52 GMT</pubDate>
        <guid>https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m</guid>
        <link>https://nitter.freedit.eu/paramonoww/status/1879500495641006082#m</link>
      </item>

  </channel>
</rss>
