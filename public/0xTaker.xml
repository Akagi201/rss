<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <atom:link href="https://nitter.freedit.eu/0xTaker/rss" rel="self" type="application/rss+xml" />
    <title>Taker / @0xTaker</title>
    <link>https://nitter.freedit.eu/0xTaker</link>
    <description>Twitter feed for: @0xTaker. Generated by nitter.freedit.eu
</description>
    <language>en-us</language>
    <ttl>40</ttl>
    <image>
      <title>Taker / @0xTaker</title>
      <link>https://nitter.freedit.eu/0xTaker</link>
      <url>https://nitter.freedit.eu/pic/pbs.twimg.com%2Fprofile_images%2F1644473849822556161%2FKDOG0F2J_400x400.jpg</url>
      <width>128</width>
      <height>128</height>
    </image>
      <item>
        <title>RT by @0xTaker: Flashblocks make Base faster

This 10x change makes Base the fastest EVM chain to date, bringing effective block times from 2 seconds down to 200 milliseconds ‚Äîand they&apos;re live on testnet right now + coming to mainnet in Q2</title>
        <dc:creator>@buildonbase</dc:creator>
        <description><![CDATA[<p>Flashblocks make Base faster<br>
<br>
This 10x change makes Base the fastest EVM chain to date, bringing effective block times from 2 seconds down to 200 milliseconds ‚Äîand they're live on testnet right now + coming to mainnet in Q2</p>
<img src="https://nitter.freedit.eu/pic/amplify_video_thumb%2F1895223201392992256%2Fimg%2F6JbcbosbXU6pbZLy.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Thu, 27 Feb 2025 21:23:27 GMT</pubDate>
        <guid>https://nitter.freedit.eu/buildonbase/status/1895223243713458184#m</guid>
        <link>https://nitter.freedit.eu/buildonbase/status/1895223243713458184#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: At a dinner last night with a bunch traders. And the dick measuring contest was asking each other whats your Coinbase fee tiers.</title>
        <dc:creator>@sui414</dc:creator>
        <description><![CDATA[<p>At a dinner last night with a bunch traders. And the dick measuring contest was asking each other whats your Coinbase fee tiers.</p>]]></description>
        <pubDate>Thu, 27 Feb 2025 18:12:15 GMT</pubDate>
        <guid>https://nitter.freedit.eu/sui414/status/1895175127261945945#m</guid>
        <link>https://nitter.freedit.eu/sui414/status/1895175127261945945#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: 1/ How do solvers like CoWSwap, 1inchFusion, and UniswapX compare with traditional DEXes like Uniswap v2 and v3?

TLDR fellow @ballsyalchemist, councilmember @sui414, and @dex_chen_V went deep on data analysis. Read on for a preview of their research!</title>
        <dc:creator>@thelatestindefi</dc:creator>
        <description><![CDATA[<p>1/ How do solvers like CoWSwap, 1inchFusion, and UniswapX compare with traditional DEXes like Uniswap v2 and v3?<br>
<br>
TLDR fellow <a href="https://nitter.freedit.eu/ballsyalchemist" title="Yuki is short, so is life">@ballsyalchemist</a>, councilmember <a href="https://nitter.freedit.eu/sui414" title="danning.eth‚ö°Ô∏èü§ñ">@sui414</a>, and <a href="https://nitter.freedit.eu/dex_chen_V" title="Dex Chenü¶áüîä">@dex_chen_V</a> went deep on data analysis. Read on for a preview of their research!</p>
<img src="https://nitter.freedit.eu/pic/media%2FGkz-TW8WcAABhwP.png" style="max-width:250px;" />]]></description>
        <pubDate>Thu, 27 Feb 2025 17:49:08 GMT</pubDate>
        <guid>https://nitter.freedit.eu/thelatestindefi/status/1895169310202437633#m</guid>
        <link>https://nitter.freedit.eu/thelatestindefi/status/1895169310202437633#m</link>
      </item>
      <item>
        <title>Goes too hard</title>
        <dc:creator>@0xTaker</dc:creator>
        <description><![CDATA[<p>Goes too hard</p>
<p><a href="https://nitter.freedit.eu/Rena_labs/status/1891506216028779008#m">nitter.freedit.eu/Rena_labs/status/1891506216028779008#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGkqzWAEWcAAxw6z.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Tue, 25 Feb 2025 23:04:03 GMT</pubDate>
        <guid>https://nitter.freedit.eu/0xTaker/status/1894523785853747497#m</guid>
        <link>https://nitter.freedit.eu/0xTaker/status/1894523785853747497#m</link>
      </item>
      <item>
        <title>If @timedotfun is to advisory like @echodotxyz and @legiondotcc is to capital, has it solved predatorial advisory offers?

We managed to get some good ones prior at @aori_io but know other teams that got fucked on what some VCs promise you vs what they acc can do</title>
        <dc:creator>@0xTaker</dc:creator>
        <description><![CDATA[<p>If <a href="https://nitter.freedit.eu/timedotfun" title="time.fun">@timedotfun</a> is to advisory like <a href="https://nitter.freedit.eu/echodotxyz" title="echo">@echodotxyz</a> and <a href="https://nitter.freedit.eu/legiondotcc" title="LEGION">@legiondotcc</a> is to capital, has it solved predatorial advisory offers?<br>
<br>
We managed to get some good ones prior at <a href="https://nitter.freedit.eu/aori_io" title="aori ‚ëÅ">@aori_io</a> but know other teams that got fucked on what some VCs promise you vs what they acc can do</p>
<p><a href="https://nitter.freedit.eu/aeyakovenko/status/1894505966437372344#m">nitter.freedit.eu/aeyakovenko/status/1894505966437372344#m</a></p>]]></description>
        <pubDate>Tue, 25 Feb 2025 22:47:36 GMT</pubDate>
        <guid>https://nitter.freedit.eu/0xTaker/status/1894519645035499621#m</guid>
        <link>https://nitter.freedit.eu/0xTaker/status/1894519645035499621#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: </title>
        <dc:creator>@YoannTurpin1</dc:creator>
        <description><![CDATA[<p></p>
<img src="https://nitter.freedit.eu/pic/media%2FGkozaJBbkAAlWIq.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Tue, 25 Feb 2025 13:45:07 GMT</pubDate>
        <guid>https://nitter.freedit.eu/YoannTurpin1/status/1894383125020774902#m</guid>
        <link>https://nitter.freedit.eu/YoannTurpin1/status/1894383125020774902#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: Holesky testnet has successfully upgraded to Pectra!üöÄ</title>
        <dc:creator>@terencechain</dc:creator>
        <description><![CDATA[<p>Holesky testnet has successfully upgraded to Pectra!üöÄ</p>
<img src="https://nitter.freedit.eu/pic/media%2FGklbP5PXoAAAvUW.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Mon, 24 Feb 2025 22:01:11 GMT</pubDate>
        <guid>https://nitter.freedit.eu/terencechain/status/1894145576276824137#m</guid>
        <link>https://nitter.freedit.eu/terencechain/status/1894145576276824137#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: We hear you, EOAs are dumb.

Pectra is deploying today.

It&apos;s almost time. EOAs are going to learn some cool new tricks.</title>
        <dc:creator>@otimlabs</dc:creator>
        <description><![CDATA[<p>We hear you, EOAs are dumb.<br>
<br>
Pectra is deploying today.<br>
<br>
It's almost time. EOAs are going to learn some cool new tricks.</p>
<img src="https://nitter.freedit.eu/pic/ext_tw_video_thumb%2F1894095029670490112%2Fpu%2Fimg%2F3x596elUAjHQG5Ni.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Mon, 24 Feb 2025 19:27:58 GMT</pubDate>
        <guid>https://nitter.freedit.eu/otimlabs/status/1894107017200975940#m</guid>
        <link>https://nitter.freedit.eu/otimlabs/status/1894107017200975940#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: According to Bloomberg, Citadel Securities, led by Ken Griffin, is planning to become a liquidity provider for cryptocurrencies. The firm aims to join the market maker roster of several exchanges, including Coinbase Global, Binance, and Crypto com, and may initially set up market-making teams outside of the United States. https://www.bloomberg.com/news/articles/2025-02-24/crypto-citadel-securities-plans-to-trade-digital-coins-on-exchanges?srnd=phx-crypto</title>
        <dc:creator>@WuBlockchain</dc:creator>
        <description><![CDATA[<p>According to Bloomberg, Citadel Securities, led by Ken Griffin, is planning to become a liquidity provider for cryptocurrencies. The firm aims to join the market maker roster of several exchanges, including Coinbase Global, Binance, and Crypto com, and may initially set up market-making teams outside of the United States. <a href="https://www.bloomberg.com/news/articles/2025-02-24/crypto-citadel-securities-plans-to-trade-digital-coins-on-exchanges?srnd=phx-crypto">bloomberg.com/news/articles/‚Ä¶</a></p>]]></description>
        <pubDate>Mon, 24 Feb 2025 17:08:17 GMT</pubDate>
        <guid>https://nitter.freedit.eu/WuBlockchain/status/1894071864122904761#m</guid>
        <link>https://nitter.freedit.eu/WuBlockchain/status/1894071864122904761#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: Open-sourcing GasFlow, a system for predicting optimal ethereum priority fees. Uses a RandomForest-based ML model trained on mempool and historical gas data to estimate fees based on network conditions. Completely open-source.</title>
        <dc:creator>@duoxehyon</dc:creator>
        <description><![CDATA[<p>Open-sourcing GasFlow, a system for predicting optimal ethereum priority fees. Uses a RandomForest-based ML model trained on mempool and historical gas data to estimate fees based on network conditions. Completely open-source.</p>
<img src="https://nitter.freedit.eu/pic/media%2FGkfpfqtaQAEnZb0.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Sun, 23 Feb 2025 19:10:34 GMT</pubDate>
        <guid>https://nitter.freedit.eu/duoxehyon/status/1893740250414846315#m</guid>
        <link>https://nitter.freedit.eu/duoxehyon/status/1893740250414846315#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: Execution should be quick and painless</title>
        <dc:creator>@0xFlooreyes</dc:creator>
        <description><![CDATA[<p>Execution should be quick and painless</p>
<img src="https://nitter.freedit.eu/pic/media%2FGkkCGvoWQAAUKDp.jpg" style="max-width:250px;" />
<img src="https://nitter.freedit.eu/pic/media%2FGkkCGvhXUAEZFTF.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Mon, 24 Feb 2025 15:31:14 GMT</pubDate>
        <guid>https://nitter.freedit.eu/0xFlooreyes/status/1894047441638265103#m</guid>
        <link>https://nitter.freedit.eu/0xFlooreyes/status/1894047441638265103#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: new aori merch just landed for http://searcher.wtf</title>
        <dc:creator>@aori_io</dc:creator>
        <description><![CDATA[<p>new aori merch just landed for <a href="http://searcher.wtf">searcher.wtf</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGkkDl80XAAAtoJS.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Mon, 24 Feb 2025 15:37:43 GMT</pubDate>
        <guid>https://nitter.freedit.eu/aori_io/status/1894049074757685479#m</guid>
        <link>https://nitter.freedit.eu/aori_io/status/1894049074757685479#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: GlueX Router API is DOMINATING! üî• 

We‚Äôre not just in the game, we are the game! 

70% of all intents on @CoWSwap  are powered by our Router API on @arbitrum 

Speed, Efficiency, Unmatched execution. That‚Äôs GlueX!</title>
        <dc:creator>@GluexProtocol</dc:creator>
        <description><![CDATA[<p>GlueX Router API is DOMINATING! üî• <br>
<br>
We‚Äôre not just in the game, we are the game! <br>
<br>
70% of all intents on <a href="https://nitter.freedit.eu/CoWSwap" title="CoW DAO">@CoWSwap</a>  are powered by our Router API on <a href="https://nitter.freedit.eu/arbitrum" title="Arbitrum">@arbitrum</a> <br>
<br>
Speed, Efficiency, Unmatched execution. That‚Äôs GlueX!</p>
<img src="https://nitter.freedit.eu/pic/media%2FGkXpfoXWAAAT6Wo.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Sat, 22 Feb 2025 05:48:54 GMT</pubDate>
        <guid>https://nitter.freedit.eu/GluexProtocol/status/1893176116195500224#m</guid>
        <link>https://nitter.freedit.eu/GluexProtocol/status/1893176116195500224#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: Something something counterparty risk</title>
        <dc:creator>@0xcarnation</dc:creator>
        <description><![CDATA[<p>Something something counterparty risk</p>
<p><a href="https://nitter.freedit.eu/benbybit/status/1892963530422505586#m">nitter.freedit.eu/benbybit/status/1892963530422505586#m</a></p>
<img src="https://nitter.freedit.eu/pic/media%2FGkUsvRAXMAAmvGa.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Fri, 21 Feb 2025 16:03:33 GMT</pubDate>
        <guid>https://nitter.freedit.eu/0xcarnation/status/1892968411761651784#m</guid>
        <link>https://nitter.freedit.eu/0xcarnation/status/1892968411761651784#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: It‚Äôs almost surreal how much time was spent during the bear market on decentralizing block building and sounding the alarm bells about the ‚Äúexistential risk‚Äù posed by MEV on the L1 and now everyone is just totally fine using a single box, like completely normalized no problem</title>
        <dc:creator>@GwartyGwart</dc:creator>
        <description><![CDATA[<p>It‚Äôs almost surreal how much time was spent during the bear market on decentralizing block building and sounding the alarm bells about the ‚Äúexistential risk‚Äù posed by MEV on the L1 and now everyone is just totally fine using a single box, like completely normalized no problem</p>]]></description>
        <pubDate>Wed, 19 Feb 2025 23:41:50 GMT</pubDate>
        <guid>https://nitter.freedit.eu/GwartyGwart/status/1892358965553451095#m</guid>
        <link>https://nitter.freedit.eu/GwartyGwart/status/1892358965553451095#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: The @0xTaker episode cooked. coming soon.</title>
        <dc:creator>@apriori0x</dc:creator>
        <description><![CDATA[<p>The <a href="https://nitter.freedit.eu/0xTaker" title="Taker">@0xTaker</a> episode cooked. coming soon.</p>]]></description>
        <pubDate>Wed, 19 Feb 2025 20:25:04 GMT</pubDate>
        <guid>https://nitter.freedit.eu/apriori0x/status/1892309447537471559#m</guid>
        <link>https://nitter.freedit.eu/apriori0x/status/1892309447537471559#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: EXP-0003: Application Subscriptions!

In this experiment we show Porto, @ithacaxyz&apos;s Ethereum Account SDK, powering subscriptions &amp; delegating access to your account securely.

Huge implications for app-builders doing payments/subscriptions, AI agents, Telegram bots, DeFi &amp; more.</title>
        <dc:creator>@gakonst</dc:creator>
        <description><![CDATA[<p>EXP-0003: Application Subscriptions!<br>
<br>
In this experiment we show Porto, <a href="https://nitter.freedit.eu/ithacaxyz" title="Ithaca">@ithacaxyz</a>'s Ethereum Account SDK, powering subscriptions &amp; delegating access to your account securely.<br>
<br>
Huge implications for app-builders doing payments/subscriptions, AI agents, Telegram bots, DeFi &amp; more.</p>
<img src="https://nitter.freedit.eu/pic/media%2FGkLBFo2bUAIy8Ij.jpg" style="max-width:250px;" />]]></description>
        <pubDate>Wed, 19 Feb 2025 18:56:16 GMT</pubDate>
        <guid>https://nitter.freedit.eu/gakonst/status/1892287101129150855#m</guid>
        <link>https://nitter.freedit.eu/gakonst/status/1892287101129150855#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: How Monad Works

Summary / Network Parameters

- Monad is EVM bytecode-equivalent (you can redeploy bytecode without recompilation)

- Cancun fork (TSTORE, TLOAD, MCOPY) is supported

- Opcode to gas units mapping is same as Ethereum (e.g. ADD is 4)

- RPC conforms to geth&apos;s RPC interface

- Blocks are every 500 ms

- Finality occurs in 1 second; finality of block N occurs at the proposal of block N+2

- Block gas limit in testnet is 150 million gas, i.e. gas rate is 300 million gas/s. This will increase over time

- 100-200 validators expected in consensus

- on Day 1 of testnet, Monad will have about 55 globally-distributed validators

Frugality / Impact on Decentralization

The driving goal of Monad is to have better software algorithms for consensus and execution, offering high performance while preserving decentralization

These algorithms deliver high performance while relying on nodes with relatively modest hardware:

- 32 GB of RAM
- 2x 2 TB SSDs
- 100 Mbps of bandwidth
- a 16-core 4.5 GHz processor like the AMD Ryzen 7950X

You can assemble this machine for about $1500

These algorithms deliver high performance while maintaining a fully-globally-distributed validator set and stake weight distribution

There isn‚Äôt a reliance on a supermajority in one geographic region - one would think this is an obvious expectation but many ‚Äúhigh-performance‚Äù L1s actually derive their performance from having a supermajority of stake weight in close proximity 

Node

Monad node has 3 components:
- monad-bft [consensus]
- monad-execution [execution + state]
- monad-rpc [handling user reads/writes]

- Network is 100-200 voting nodes (we‚Äôll call them ‚Äúvalidators‚Äù for the rest of this doc)

- Non-voting full nodes listen to network traffic

- All nodes execute all transactions and have full state

Consensus Mechanism

Overall consensus mechanism is MonadBFT.

MonadBFT has linear communication complexity which allows it to scale to far more nodes than quadratic-complexity algorithms like CometBFT

In the happy path, it follows the pattern of ‚Äúone-to-many-to-one‚Äù or ‚Äúfan out, fan in‚Äù:
 
- Leader (Alice) broadcasts a signed block proposal to all other nodes (fan out), who acknowledge its validity by sending a signed attestation the next leader Bob (fan in). 

- Bob aggregates the attestations into a ‚ÄúQuorum Certificate‚Äù (QC)

- Attestation signatures use the BLS signature scheme for ease of aggregation

- Bob broadcasts the QC to all the nodes, who attest to receiving it by sending a message to the 3rd leader (Charlie) who aggregates the attestations into a QC-on-QC

- Charlie sends the QC-on-QC to everyone. Upon receiving the QC-on-QC, everyone knows that Alice‚Äôs block has been finalized

In the above story, Bob and Charlie are only sending out QCs or QCs-on-QCs, but in reality the proposals are pipelined: 

- Bob‚Äôs message contains both the QC for Alice‚Äôs block and also the contents of a new block. 

- Charlie‚Äôs message contains the QC for Bob‚Äôs block (which is a QC-on-QC for Alice‚Äôs block) and also contains the transactions for a new block 

When validators send an attestation for Bob‚Äôs message they are attesting to both the validity of Bob‚Äôs block and the validity of the QC

This pipelining raises the throughput of the network since every slot a new block gets produced.

The below diagram shows how MonadBFT reaches consensus. Pipelining is tracked at the top:

See the docs for a fuller description. Obvious questions addressed there are:

- How the network handles the unhappy path where Bob doesn‚Äôt get enough a supermajority of attestations

- How the above mechanism results in nodes being sure that the block has been finalized once they have received the QC-on-QC

RaptorCast

MonadBFT requires the leader to directly send blocks to every validator

However, blocks may be quite large: 10,000 transactions/s * 200 bytes/tx = 2 MB/s. Sending directly to 200 validators would require 400 MB/s (3.2 Gbps). 

We don‚Äôt want validators to have to have such high upload bandwidth

RaptorCast is a specialized messaging protocol which solves this problem

In RaptorCast, a block is erasure-coded to produce a bunch of smaller chunks

In erasure coding, the total size of all of the chunks is greater than the original data (by a multiplicative factor) but the original data can be restored using (almost) any combination of chunks whose total size matches the original data‚Äôs size

For example, a 1000 kb block erasure-coded with a multiplicative factor of 3 might produce 150 20kb chunks, but (roughly) any 50 of the chunks can reassemble the original message

RaptorCast uses a variant of Raptor codes as the encoding mechanism

In RaptorCast, each chunk is sent to one validator who is tasked with sending the chunk to every other validator in the network

That is, each chunk follows a two-level broadcast tree where the leader is the root, one other validator is at the first level, and all other validators are on the second level

Validators are assigned chunks prorata to their stake weight

Here&apos;s a diagram showing the RaptorCast protocol: each validator serves as a first-hop recipient for a range of chunks, and broadcasts those chunks to every other validator:

Raptorcast properties:

- Using the two-level broadcast tree ensures that message delivery occurs within 2x the longest hop

- Upload bandwidth for the leader is limited to the block size times the replication factor (roughly 2)

- Since chunks are assigned pro-rata to stake weight, and BFT assumes no more than 33% of stake is malicious, at most 33% of chunks could fail to reach their recipients. With a replication factor of 2x, nodes can reconstruct the original block despite a maximum 33% loss.

Transaction Lifecycle

- User submits pending transaction to RPC node

- RPC node sends pending transaction to next 3 leaders based on the leader schedule

- Pending transaction gets added to those leaders‚Äô local mempools

- Leader adds transaction to their block as they see fit [default: they order by descending fee-per-gas-unit, i.e. Priority Gas Auction]

- Leader proposes block, which is confirmed by the network as mentioned above

Note: directly forwarding to upcoming leaders (as opposed to flood forwarding to all nodes) greatly reduces traffic. Flood forwarding would take up the entire bandwidth

Note: in the future, a behavior is being considered where leaders forward pending transactions (that they weren‚Äôt able to include in their block) to the next leader

Leader Election

- Leaders in the current testnet are permissioned. Staking will be added shortly

- An epoch occurs roughly once per day. Validator stake weights are locked in one epoch ahead (i.e. any changes for epoch N+1 must be registered prior to the start of epoch N)

- At the start of each epoch, each validator computes the leader schedule based on running a deterministic pseudorandom function on the stake weights. Since the function is deterministic everyone arrives at the same leader schedule

Asynchronous Execution

Monad pipelines consensus and execution, moving execution out of the hot path of consensus into a separate swim lane and allowing execution to utilize the full block time.

- Consensus is reached prior to execution 

- Leader &amp; validators check transaction validity (valid signature; valid nonce; submitter can pay for the data cost of the transaction being transmitted), but are not required to execute the transactions prior to voting.

- After a block is finalized, it is executed; meanwhile consensus is already proceeding on subsequent blocks

This is in contrast to most blockchains, which have interleaved execution.

One way to understand the impact of asynchronous execution is to recognize that, in interleaved execution, the execution budget is necessarily a small fraction of the block time since in interleaved execution, the leader must execute the transactions before proposing the block, and validators must execute before responding

For a 500 ms block time, almost all of the time will be budgeted for multiple rounds of cross-globe communication, leaving only a small fraction of the time for execution

The below diagram contrasts interleaved execution with asynchronous execution. Blue rectangles correspond to time spent on execution while orange rectangles correspond to time spent on consensus.

The budget for execution is much larger in async execution.

Delayed merkle root

Due to async execution, Monad block proposals don‚Äôt include the merkle root of the state trie, since that would require execution to have already completed.

All nodes should stay in sync because they‚Äôre all starting from the same point and doing the same work. 

But it‚Äôd be nice to be sure! As a precaution, proposals in Monad also include a delayed merkle root from D blocks ago, allowing nodes to detect if they have diverged. D is a systemwide parameter, currently set to 3. 

If any of the validators makes a computation error (cosmic rays?) when computing the state root at block N, it will realize that it possibly erred by block N+D (since the delayed merkle root for N contained in that block differs from its local view). 
The validator then needs to wait until N+D+2 to see if 2/3 of the stake weight finalizes the block proposal at N+D (in which case the local node made an error) or if the block gets rejected (in which case the leader made the error).

Block stages

Assume that a validator has just received block N. We say that:

- Block N is ‚Äòproposed‚Äô

- Block N-1 is ‚Äòvoted‚Äô

- Block N-2 is ‚Äòfinalized‚Äô (because block N carries the QC-on-QC of block N-2)

- Block N-2-D is ‚Äòverified‚Äô (because block N-2 carries the merkle root post the transactions in block N-2-D, and block N-2 is the last block that has been finalized)

Note that unlike Ethereum, only one block at height N is proposed and voted on, avoiding retroactive block reorganization due to competing forks

Speculative execution

Although only block N-2 is ‚Äòfinalized‚Äô and can officially be executed, nodes have a strong suspicion that the lists of transactions in block N-1 and block N are likely to become the finalized lists

Therefore, nodes speculatively execute the transactions included in each new proposed block, storing a pointer to the state trie post those transactions. In the event that a block ends up not being finalized, the pointer is discarded, undoing the execution

Speculative execution allows nodes to (likely) have the most up-to-date state, which helps users simulate transactions correctly

Optimistic parallel execution

Like in Ethereum, blocks are linearly ordered, as are transactions. That means that the true state of the world is the state arrived at by executing all transactions one after another

In Monad, transactions are executed optimistically in parallel to generate pending results. A pending result contains the list of storage slots that were read (SLOADed) and written (SSTOREd) during the course of that execution. We refer to these slots as ‚Äúinputs‚Äù and ‚Äúoutputs‚Äù

Pending results are committed serially, checking that each pending result‚Äôs inputs are still valid, and re-executing if an input has been invalidated. This serial commitment ensures that the result is the same as if the transactions were executed serially

Here&apos;s an example of how Optimistic Parallel Execution works:

Assume that prior to the start of a block, the following are the USDC balances:
- Alice: 1000 USDC
- Bob: 0 USDC
- Charlie: 400 USD
(Note also that each of these balances corresponds to 1 storage slot, since each is 1 key-value pair in a mapping in the USDC contract.)

Two transactions appear as transaction 0 and 1 in the block:
- Transaction 0: Alice sends 100 USDC to Bob
- Transaction 1: Alice sends 100 USDC to Charlie

Then optimistic parallel execution will produce two pending results:

- PendingResult0: 
  * Inputs: Alice = 1000 USDC, Bob = 0 USDC
  * Outputs: Alice = 900 USDC; Bob = 100 USDC

- PendingResult 1: 
  * Inputs: Alice = 1000 USDC; Charlie = 400 USDC
  * Outputs: Alice = 900 USDC, Charlie = 500 USDC

When we go to commit these pending results:

- PendingResult 0 is committed successfully, changing the official state to Alice = 900, Bob = 100, Charlie = 400

- PendingResult 1 cannot be committed because now one of the inputs conflicts (Alice was assumed to have 1000, but actually has 900)

So transaction 1 is re-executed

Final result:
- Alice: 800 USDC
- Bob: 100 USDC
- Charlie: 500 USDC

Note that in optimistic parallel execution, every transaction gets executed at most twice - once optimistically, and (at most) once when it is being committed

Re-execution is typically cheap because storage slots are usually in cache. It is only when re-execution triggers a different codepath (requiring a different slot) that execution has to read a storage slot from SSD

MonadDb

As in Ethereum, state is stored in a merkle trie. There is a custom database, MonadDb, which stores merkle trie data natively

This differs from existing clients [which embed the merkle trie inside of a commodity database which itself uses a tree structure]

MonadDb is a significant optimization because it eliminates a level of indirection, reduces the number of pages read from SSD in order to perform one lookup, allows for async I/O, and allows the filesystem to be bypassed

State access [SLOAD and SSTORE] is the biggest bottleneck for execution, and MonadDb is a significant unlock for state access because:
- it reduces the number of iops to read or write one value
- it makes recomputing the merkle root a lot faster
- and it supports many parallel reads which the parallel execution system can take advantage of

Synergies between optimistic parallel execution and MonadDb

Optimistic parallel execution can be thought of as surfacing many storage slot dependencies ‚Äì all of the inputs and outputs of the pending results ‚Äì in parallel and pulling them into the cache

Even in the worst case where every pending result‚Äôs inputs are invalidated and the transaction has to be re-executed, optimistic parallel execution is still extremely useful by ‚Äúrunning ahead‚Äù of the serial commitment and pulling many storage slots from SSD

This makes optimistic parallel execution and MonadDb work really well together, because MonadDb provides fast asynchronous state lookups while optimistic parallel execution cues up many parallel reads from SSD

Bootstrapping a node (Statesync/Blocksync)

High throughput means a long transaction history which makes replaying from genesis challenging

Most node operators will prefer to initialize their nodes by copying over recent state from other nodes and only replaying the last mile. This is what statesync accomplishes

In statesync, a synchronizing node (‚Äúclient‚Äù) provides their current view‚Äôs version and a target version and asks other nodes (‚Äúservers‚Äù) to help it progress from the current view to the target version

MonadDb has versioning on each node in the trie. Servers use this version information to identify which trie components need to be sent

Nodes can also request blocks from their peers in a protocol called blocksync. This is used if a block is missed (not enough chunks arrived), as well as when executing the ‚Äúlast mile‚Äù after statesync completes (since more blocks will have come in since the start of statesync)

Thanks for reading and be sure to check out the docs</title>
        <dc:creator>@keoneHD</dc:creator>
        <description><![CDATA[<p>How Monad Works<br>
<br>
Summary / Network Parameters<br>
<br>
- Monad is EVM bytecode-equivalent (you can redeploy bytecode without recompilation)<br>
<br>
- Cancun fork (TSTORE, TLOAD, MCOPY) is supported<br>
<br>
- Opcode to gas units mapping is same as Ethereum (e.g. ADD is 4)<br>
<br>
- RPC conforms to geth's RPC interface<br>
<br>
- Blocks are every 500 ms<br>
<br>
- Finality occurs in 1 second; finality of block N occurs at the proposal of block N+2<br>
<br>
- Block gas limit in testnet is 150 million gas, i.e. gas rate is 300 million gas/s. This will increase over time<br>
<br>
- 100-200 validators expected in consensus<br>
<br>
- on Day 1 of testnet, Monad will have about 55 globally-distributed validators<br>
<br>
Frugality / Impact on Decentralization<br>
<br>
The driving goal of Monad is to have better software algorithms for consensus and execution, offering high performance while preserving decentralization<br>
<br>
These algorithms deliver high performance while relying on nodes with relatively modest hardware:<br>
<br>
- 32 GB of RAM<br>
- 2x 2 TB SSDs<br>
- 100 Mbps of bandwidth<br>
- a 16-core 4.5 GHz processor like the AMD Ryzen 7950X<br>
<br>
You can assemble this machine for about $1500<br>
<br>
These algorithms deliver high performance while maintaining a fully-globally-distributed validator set and stake weight distribution<br>
<br>
There isn‚Äôt a reliance on a supermajority in one geographic region - one would think this is an obvious expectation but many ‚Äúhigh-performance‚Äù L1s actually derive their performance from having a supermajority of stake weight in close proximity <br>
<br>
Node<br>
<br>
Monad node has 3 components:<br>
- monad-bft [consensus]<br>
- monad-execution [execution + state]<br>
- monad-rpc [handling user reads/writes]<br>
<br>
- Network is 100-200 voting nodes (we‚Äôll call them ‚Äúvalidators‚Äù for the rest of this doc)<br>
<br>
- Non-voting full nodes listen to network traffic<br>
<br>
- All nodes execute all transactions and have full state<br>
<br>
Consensus Mechanism<br>
<br>
Overall consensus mechanism is MonadBFT.<br>
<br>
MonadBFT has linear communication complexity which allows it to scale to far more nodes than quadratic-complexity algorithms like CometBFT<br>
<br>
In the happy path, it follows the pattern of ‚Äúone-to-many-to-one‚Äù or ‚Äúfan out, fan in‚Äù:<br>
 <br>
- Leader (Alice) broadcasts a signed block proposal to all other nodes (fan out), who acknowledge its validity by sending a signed attestation the next leader Bob (fan in). <br>
<br>
- Bob aggregates the attestations into a ‚ÄúQuorum Certificate‚Äù (QC)<br>
<br>
- Attestation signatures use the BLS signature scheme for ease of aggregation<br>
<br>
- Bob broadcasts the QC to all the nodes, who attest to receiving it by sending a message to the 3rd leader (Charlie) who aggregates the attestations into a QC-on-QC<br>
<br>
- Charlie sends the QC-on-QC to everyone. Upon receiving the QC-on-QC, everyone knows that Alice‚Äôs block has been finalized<br>
<br>
In the above story, Bob and Charlie are only sending out QCs or QCs-on-QCs, but in reality the proposals are pipelined: <br>
<br>
- Bob‚Äôs message contains both the QC for Alice‚Äôs block and also the contents of a new block. <br>
<br>
- Charlie‚Äôs message contains the QC for Bob‚Äôs block (which is a QC-on-QC for Alice‚Äôs block) and also contains the transactions for a new block <br>
<br>
When validators send an attestation for Bob‚Äôs message they are attesting to both the validity of Bob‚Äôs block and the validity of the QC<br>
<br>
This pipelining raises the throughput of the network since every slot a new block gets produced.<br>
<br>
The below diagram shows how MonadBFT reaches consensus. Pipelining is tracked at the top:<br>
<br>
See the docs for a fuller description. Obvious questions addressed there are:<br>
<br>
- How the network handles the unhappy path where Bob doesn‚Äôt get enough a supermajority of attestations<br>
<br>
- How the above mechanism results in nodes being sure that the block has been finalized once they have received the QC-on-QC<br>
<br>
RaptorCast<br>
<br>
MonadBFT requires the leader to directly send blocks to every validator<br>
<br>
However, blocks may be quite large: 10,000 transactions/s * 200 bytes/tx = 2 MB/s. Sending directly to 200 validators would require 400 MB/s (3.2 Gbps). <br>
<br>
We don‚Äôt want validators to have to have such high upload bandwidth<br>
<br>
RaptorCast is a specialized messaging protocol which solves this problem<br>
<br>
In RaptorCast, a block is erasure-coded to produce a bunch of smaller chunks<br>
<br>
In erasure coding, the total size of all of the chunks is greater than the original data (by a multiplicative factor) but the original data can be restored using (almost) any combination of chunks whose total size matches the original data‚Äôs size<br>
<br>
For example, a 1000 kb block erasure-coded with a multiplicative factor of 3 might produce 150 20kb chunks, but (roughly) any 50 of the chunks can reassemble the original message<br>
<br>
RaptorCast uses a variant of Raptor codes as the encoding mechanism<br>
<br>
In RaptorCast, each chunk is sent to one validator who is tasked with sending the chunk to every other validator in the network<br>
<br>
That is, each chunk follows a two-level broadcast tree where the leader is the root, one other validator is at the first level, and all other validators are on the second level<br>
<br>
Validators are assigned chunks prorata to their stake weight<br>
<br>
Here's a diagram showing the RaptorCast protocol: each validator serves as a first-hop recipient for a range of chunks, and broadcasts those chunks to every other validator:<br>
<br>
Raptorcast properties:<br>
<br>
- Using the two-level broadcast tree ensures that message delivery occurs within 2x the longest hop<br>
<br>
- Upload bandwidth for the leader is limited to the block size times the replication factor (roughly 2)<br>
<br>
- Since chunks are assigned pro-rata to stake weight, and BFT assumes no more than 33% of stake is malicious, at most 33% of chunks could fail to reach their recipients. With a replication factor of 2x, nodes can reconstruct the original block despite a maximum 33% loss.<br>
<br>
Transaction Lifecycle<br>
<br>
- User submits pending transaction to RPC node<br>
<br>
- RPC node sends pending transaction to next 3 leaders based on the leader schedule<br>
<br>
- Pending transaction gets added to those leaders‚Äô local mempools<br>
<br>
- Leader adds transaction to their block as they see fit [default: they order by descending fee-per-gas-unit, i.e. Priority Gas Auction]<br>
<br>
- Leader proposes block, which is confirmed by the network as mentioned above<br>
<br>
Note: directly forwarding to upcoming leaders (as opposed to flood forwarding to all nodes) greatly reduces traffic. Flood forwarding would take up the entire bandwidth<br>
<br>
Note: in the future, a behavior is being considered where leaders forward pending transactions (that they weren‚Äôt able to include in their block) to the next leader<br>
<br>
Leader Election<br>
<br>
- Leaders in the current testnet are permissioned. Staking will be added shortly<br>
<br>
- An epoch occurs roughly once per day. Validator stake weights are locked in one epoch ahead (i.e. any changes for epoch N+1 must be registered prior to the start of epoch N)<br>
<br>
- At the start of each epoch, each validator computes the leader schedule based on running a deterministic pseudorandom function on the stake weights. Since the function is deterministic everyone arrives at the same leader schedule<br>
<br>
Asynchronous Execution<br>
<br>
Monad pipelines consensus and execution, moving execution out of the hot path of consensus into a separate swim lane and allowing execution to utilize the full block time.<br>
<br>
- Consensus is reached prior to execution <br>
<br>
- Leader & validators check transaction validity (valid signature; valid nonce; submitter can pay for the data cost of the transaction being transmitted), but are not required to execute the transactions prior to voting.<br>
<br>
- After a block is finalized, it is executed; meanwhile consensus is already proceeding on subsequent blocks<br>
<br>
This is in contrast to most blockchains, which have interleaved execution.<br>
<br>
One way to understand the impact of asynchronous execution is to recognize that, in interleaved execution, the execution budget is necessarily a small fraction of the block time since in interleaved execution, the leader must execute the transactions before proposing the block, and validators must execute before responding<br>
<br>
For a 500 ms block time, almost all of the time will be budgeted for multiple rounds of cross-globe communication, leaving only a small fraction of the time for execution<br>
<br>
The below diagram contrasts interleaved execution with asynchronous execution. Blue rectangles correspond to time spent on execution while orange rectangles correspond to time spent on consensus.<br>
<br>
The budget for execution is much larger in async execution.<br>
<br>
Delayed merkle root<br>
<br>
Due to async execution, Monad block proposals don‚Äôt include the merkle root of the state trie, since that would require execution to have already completed.<br>
<br>
All nodes should stay in sync because they‚Äôre all starting from the same point and doing the same work. <br>
<br>
But it‚Äôd be nice to be sure! As a precaution, proposals in Monad also include a delayed merkle root from D blocks ago, allowing nodes to detect if they have diverged. D is a systemwide parameter, currently set to 3. <br>
<br>
If any of the validators makes a computation error (cosmic rays?) when computing the state root at block N, it will realize that it possibly erred by block N+D (since the delayed merkle root for N contained in that block differs from its local view). <br>
The validator then needs to wait until N+D+2 to see if 2/3 of the stake weight finalizes the block proposal at N+D (in which case the local node made an error) or if the block gets rejected (in which case the leader made the error).<br>
<br>
Block stages<br>
<br>
Assume that a validator has just received block N. We say that:<br>
<br>
- Block N is ‚Äòproposed‚Äô<br>
<br>
- Block N-1 is ‚Äòvoted‚Äô<br>
<br>
- Block N-2 is ‚Äòfinalized‚Äô (because block N carries the QC-on-QC of block N-2)<br>
<br>
- Block N-2-D is ‚Äòverified‚Äô (because block N-2 carries the merkle root post the transactions in block N-2-D, and block N-2 is the last block that has been finalized)<br>
<br>
Note that unlike Ethereum, only one block at height N is proposed and voted on, avoiding retroactive block reorganization due to competing forks<br>
<br>
Speculative execution<br>
<br>
Although only block N-2 is ‚Äòfinalized‚Äô and can officially be executed, nodes have a strong suspicion that the lists of transactions in block N-1 and block N are likely to become the finalized lists<br>
<br>
Therefore, nodes speculatively execute the transactions included in each new proposed block, storing a pointer to the state trie post those transactions. In the event that a block ends up not being finalized, the pointer is discarded, undoing the execution<br>
<br>
Speculative execution allows nodes to (likely) have the most up-to-date state, which helps users simulate transactions correctly<br>
<br>
Optimistic parallel execution<br>
<br>
Like in Ethereum, blocks are linearly ordered, as are transactions. That means that the true state of the world is the state arrived at by executing all transactions one after another<br>
<br>
In Monad, transactions are executed optimistically in parallel to generate pending results. A pending result contains the list of storage slots that were read (SLOADed) and written (SSTOREd) during the course of that execution. We refer to these slots as ‚Äúinputs‚Äù and ‚Äúoutputs‚Äù<br>
<br>
Pending results are committed serially, checking that each pending result‚Äôs inputs are still valid, and re-executing if an input has been invalidated. This serial commitment ensures that the result is the same as if the transactions were executed serially<br>
<br>
Here's an example of how Optimistic Parallel Execution works:<br>
<br>
Assume that prior to the start of a block, the following are the USDC balances:<br>
- Alice: 1000 USDC<br>
- Bob: 0 USDC<br>
- Charlie: 400 USD<br>
(Note also that each of these balances corresponds to 1 storage slot, since each is 1 key-value pair in a mapping in the USDC contract.)<br>
<br>
Two transactions appear as transaction 0 and 1 in the block:<br>
- Transaction 0: Alice sends 100 USDC to Bob<br>
- Transaction 1: Alice sends 100 USDC to Charlie<br>
<br>
Then optimistic parallel execution will produce two pending results:<br>
<br>
- PendingResult0: <br>
  * Inputs: Alice = 1000 USDC, Bob = 0 USDC<br>
  * Outputs: Alice = 900 USDC; Bob = 100 USDC<br>
<br>
- PendingResult 1: <br>
  * Inputs: Alice = 1000 USDC; Charlie = 400 USDC<br>
  * Outputs: Alice = 900 USDC, Charlie = 500 USDC<br>
<br>
When we go to commit these pending results:<br>
<br>
- PendingResult 0 is committed successfully, changing the official state to Alice = 900, Bob = 100, Charlie = 400<br>
<br>
- PendingResult 1 cannot be committed because now one of the inputs conflicts (Alice was assumed to have 1000, but actually has 900)<br>
<br>
So transaction 1 is re-executed<br>
<br>
Final result:<br>
- Alice: 800 USDC<br>
- Bob: 100 USDC<br>
- Charlie: 500 USDC<br>
<br>
Note that in optimistic parallel execution, every transaction gets executed at most twice - once optimistically, and (at most) once when it is being committed<br>
<br>
Re-execution is typically cheap because storage slots are usually in cache. It is only when re-execution triggers a different codepath (requiring a different slot) that execution has to read a storage slot from SSD<br>
<br>
MonadDb<br>
<br>
As in Ethereum, state is stored in a merkle trie. There is a custom database, MonadDb, which stores merkle trie data natively<br>
<br>
This differs from existing clients [which embed the merkle trie inside of a commodity database which itself uses a tree structure]<br>
<br>
MonadDb is a significant optimization because it eliminates a level of indirection, reduces the number of pages read from SSD in order to perform one lookup, allows for async I/O, and allows the filesystem to be bypassed<br>
<br>
State access [SLOAD and SSTORE] is the biggest bottleneck for execution, and MonadDb is a significant unlock for state access because:<br>
- it reduces the number of iops to read or write one value<br>
- it makes recomputing the merkle root a lot faster<br>
- and it supports many parallel reads which the parallel execution system can take advantage of<br>
<br>
Synergies between optimistic parallel execution and MonadDb<br>
<br>
Optimistic parallel execution can be thought of as surfacing many storage slot dependencies ‚Äì all of the inputs and outputs of the pending results ‚Äì in parallel and pulling them into the cache<br>
<br>
Even in the worst case where every pending result‚Äôs inputs are invalidated and the transaction has to be re-executed, optimistic parallel execution is still extremely useful by ‚Äúrunning ahead‚Äù of the serial commitment and pulling many storage slots from SSD<br>
<br>
This makes optimistic parallel execution and MonadDb work really well together, because MonadDb provides fast asynchronous state lookups while optimistic parallel execution cues up many parallel reads from SSD<br>
<br>
Bootstrapping a node (Statesync/Blocksync)<br>
<br>
High throughput means a long transaction history which makes replaying from genesis challenging<br>
<br>
Most node operators will prefer to initialize their nodes by copying over recent state from other nodes and only replaying the last mile. This is what statesync accomplishes<br>
<br>
In statesync, a synchronizing node (‚Äúclient‚Äù) provides their current view‚Äôs version and a target version and asks other nodes (‚Äúservers‚Äù) to help it progress from the current view to the target version<br>
<br>
MonadDb has versioning on each node in the trie. Servers use this version information to identify which trie components need to be sent<br>
<br>
Nodes can also request blocks from their peers in a protocol called blocksync. This is used if a block is missed (not enough chunks arrived), as well as when executing the ‚Äúlast mile‚Äù after statesync completes (since more blocks will have come in since the start of statesync)<br>
<br>
Thanks for reading and be sure to check out the docs</p>
<img src="https://nitter.freedit.eu/pic/media%2FGkDI-oeWoAA03yZ.jpg" style="max-width:250px;" />
<img src="https://nitter.freedit.eu/pic/media%2FGkDJwUcXMAEPk43.png" style="max-width:250px;" />
<img src="https://nitter.freedit.eu/pic/media%2FGkDKUriXYAA91Hy.jpg" style="max-width:250px;" />
<img src="https://nitter.freedit.eu/pic/media%2FGkDM1jcWgAAHL1j.png" style="max-width:250px;" />]]></description>
        <pubDate>Tue, 18 Feb 2025 06:37:38 GMT</pubDate>
        <guid>https://nitter.freedit.eu/keoneHD/status/1891738830534766710#m</guid>
        <link>https://nitter.freedit.eu/keoneHD/status/1891738830534766710#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: March 30 - April 4th, 2025 ‚ù§Ô∏è‚Äçüî•üáÆüáπ we are coming back better than ever</title>
        <dc:creator>@solidityslayer</dc:creator>
        <description><![CDATA[<p>March 30 - April 4th, 2025 ‚ù§Ô∏è‚Äçüî•üáÆüáπ we are coming back better than ever</p>
<p><a href="https://nitter.freedit.eu/ethaly_io/status/1892050349974053122#m">nitter.freedit.eu/ethaly_io/status/1892050349974053122#m</a></p>]]></description>
        <pubDate>Wed, 19 Feb 2025 03:21:20 GMT</pubDate>
        <guid>https://nitter.freedit.eu/solidityslayer/status/1892051817523019874#m</guid>
        <link>https://nitter.freedit.eu/solidityslayer/status/1892051817523019874#m</link>
      </item>
      <item>
        <title>RT by @0xTaker: JOIN ME FOR CAMPARI AND LATE NIGHT HACKING</title>
        <dc:creator>@jmininger0</dc:creator>
        <description><![CDATA[<p>JOIN ME FOR CAMPARI AND LATE NIGHT HACKING</p>
<p><a href="https://nitter.freedit.eu/ethaly_io/status/1892050349974053122#m">nitter.freedit.eu/ethaly_io/status/1892050349974053122#m</a></p>]]></description>
        <pubDate>Wed, 19 Feb 2025 03:23:28 GMT</pubDate>
        <guid>https://nitter.freedit.eu/jmininger0/status/1892052354842739026#m</guid>
        <link>https://nitter.freedit.eu/jmininger0/status/1892052354842739026#m</link>
      </item>

  </channel>
</rss>
